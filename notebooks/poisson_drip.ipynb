{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Drip Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import Bounds, minimize\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.special import factorial\n",
    "from scipy import integrate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../python/')\n",
    "import util\n",
    "from ephys import Poisson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create handling class\n",
    "Moved to `ephys` module. See `../python/ephys.py` for implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test inter-event time sampling\n",
    "This should be drawn from an exponential distribution: $t \\sim \\lambda e^{-\\lambda t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "lam = 0.5\n",
    "poisson = Poisson(lam, homogeneous=True)\n",
    "\n",
    "# Get inter-event times\n",
    "sample = poisson.times(n=10000, interevent=True)\n",
    "\n",
    "# Print results\n",
    "print('Expected mean: %.5e' % (1.0/lam))\n",
    "print('Actual mean: %.5e' % (np.mean(sample)))\n",
    "print('Expected std: %.5e' % (1.0/lam))\n",
    "print('Actual std: %.5e' % (np.std(sample)))\n",
    "\n",
    "# Plot results\n",
    "plt.hist(sample, bins=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test event sampling\n",
    "This should be drawn from the Poisson distribution: $N(s) \\sim Poisson(\\lambda s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "lam = 2.0\n",
    "s = 5.0\n",
    "iters = 1000\n",
    "\n",
    "# Create distribution\n",
    "poisson = Poisson(lam, homogeneous=True)\n",
    "\n",
    "# Sample number of events\n",
    "sample = np.zeros(iters, dtype=np.int64)\n",
    "for i in range(iters):\n",
    "    sample[i] = poisson.events(s)\n",
    "    \n",
    "# Get probabilities of events samples\n",
    "n_range = np.arange(2*lam*s)\n",
    "probs = poisson.P(n=n_range, s=s)\n",
    "\n",
    "# Print results\n",
    "print('Expected mean: %.5f' % (lam*s))\n",
    "print('Actual mean: %.5f' % (np.mean(sample)))\n",
    "print('Expected std: %.5f' % ((lam*s)**0.5))\n",
    "print('Actual std: %.5f' % (np.std(sample)))\n",
    "\n",
    "# Plot results\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(sample, bins=np.arange(np.max(sample))+0.5, color='C1', alpha=0.5)\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(n_range, probs, color='C7', alpha=0.75);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search parameter space\n",
    "It might be best to solve for the optimal foraging behavior (e.g. residence time) given a specified environment (e.g. travel time, cumulative reward function). Here, the cumulative reward function is actually based on a non-homogeneous Poisson process in the drip function.\n",
    "\n",
    "Recall that a Poisson process is given by:\n",
    "- exponentially distributed variables $t \\mid p(t) = \\lambda e^{-\\lambda t}$\n",
    "- a random variable $T \\mid T_n = t_1 + t_2 + \\ldots + t_n$\n",
    "- a number of events $N \\mid N(s) = max\\{n \\mid T_n \\leq s\\}$\n",
    "\n",
    "The underlying statistics of such a process are:\n",
    "\n",
    "$\n",
    "\\quad P(N(s) = n) = e^{-\\lambda s} \\frac{(\\lambda s)^n}{n!} = Poisson(\\lambda s) \\\\\n",
    "\\quad \\mathbb{E}[N(s)] = \\lambda s \\\\\n",
    "\\quad var[N(s)] = \\lambda s\n",
    "$\n",
    "\n",
    "(We will use $p$ for probability density and $P$ for cumulative probability density. See Appendix for derivation of these statistics.)\n",
    "\n",
    "When $\\lambda$ is constant, the above is termed a *homogeneous* Poisson process. However, if $\\lambda$ varies over time, then the process becomes *non-homogeneous* or *inhomogeneous*. In such cases, the process can be modeled as a homogeneous Poisson process over some interval $\\left [ t_1, t_2 \\right ]$ by replacing $\\lambda$ with its integral over that time interval:\n",
    "\n",
    "$\n",
    "\\quad \\Lambda(t, s) = \\int_t^{t+s} \\lambda(t) dt \\\\\n",
    "\\quad P_t(N(s) = n) = e^{-\\Lambda(t, s)} \\frac{(\\Lambda(t, s))^n}{n!} = Poisson(\\Lambda(t, s)) \\\\\n",
    "\\quad \\mathbb{E}_t[N(s)] = \\Lambda(t, s) \\\\\n",
    "\\quad var_t[N(s)] = \\Lambda(t, s)\n",
    "$\n",
    "\n",
    "(See Appendix for derivations.)\n",
    "\n",
    "Given that drip times follow $T \\sim Poisson(\\Lambda(t, s))$, the mean drip rate is $\\frac{\\mathbb{E}[N(s)]}{s} = \\frac{\\Lambda(t, s)}{s}$ (or conversely, the mean inter-drip time is given by $\\mathbb{E}_t (t_i - t_{i-1}) = \\frac{1}{\\Lambda(t, s)}$). The instantaneous fill rate of the bucket is then $\\frac{dV}{dt} = lim_{s \\to 0} \\left ( V_{drip} \\Lambda(t, s) \\right ) = V_{drip} \\lambda(t)$, and the cumulative reward function since time $t$ is:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\quad V \n",
    "&= V_{drip} \\mathbb{E}_t[N(s)] \\\\\n",
    "&= V_{drip} \\Lambda(t, s) \\\\\n",
    "&= V_{drip} \\int_t^{t+s} \\lambda(t') dt'\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "In this case, $\\lambda(t)$ decays over time:\n",
    "\n",
    "$\\quad \\lambda(t) = \\lambda_0 e^{-\\frac{t}{\\tau}}$\n",
    "\n",
    "So the cumulative reward functions becomes:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\quad V &= V_{drip} \\int_t^{t+s} \\lambda_0 e^{-\\frac{t'}{\\tau}} dt' \\\\\n",
    "&= \\lambda_0 V_{drip} \\int_t^{t+s} e^{-\\frac{t'}{\\tau}} dt' \\\\\n",
    "&= \\lambda_0 V_{drip} \\tau \\left ( e^{-\\frac{t}{\\tau}} - e^{-\\frac{t+s}{\\tau}} \\right )\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "or if $t=0$:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\quad V &= \\lambda_0 V_{drip} \\tau \\left ( 1 - e^{-\\frac{s}{\\tau}} \\right )\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Looking at the solution to the marginal value theorem for an exponentially decaying reward rate, we see that, on average, the Poisson drip model is in fact equivalent, with $r_0 = \\lambda_0 V_{drip}$. Thus searching for parameters in this model amounts to finding the right parameters in the deterministic model ($r_0, R_0, \\tau$), and then tuning the Poisson model parameters ($\\lambda_0, V_{drip}$) within the constraint that $\\lambda_0 V_{drip} = r_0$. While the expected cumulative reward per patch will remain the same, the variance of the cumulative reward per patch will vary as $\\lambda_0 V^2_{drip}$: \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\quad \\mathbb{E}_t[V(s)] \n",
    "&= \\mathbb{E}_t[V_{drip} N(s)] \\\\\n",
    "&= V_{drip} \\mathbb{E}_t[N(s)] \\\\\n",
    "&= V_{drip} \\Lambda(t, s) \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\quad var_t[V(s)] \n",
    "&= var_t[V_{drip} N(s)] \\\\\n",
    "&= \\mathbb{E}_t[(V_{drip} N(s))^2] - \\left ( \\mathbb{E}_t[V_{drip} N(s)] \\right )^2 \\\\\n",
    "&= V^2_{drip} \\mathbb{E}_t[(N(s))^2] - \\left ( V_{drip} \\mathbb{E}_t[N(s)] \\right )^2 \\\\\n",
    "&= V^2_{drip} \\left ( \\mathbb{E}_t[(N(s))^2] - \\left (\\mathbb{E}_t[N(s)] \\right)^2 \\right ) \\\\\n",
    "&= V^2_{drip} \\Lambda(t, s)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "(See the Appendix for another proof of the variance.)\n",
    "\n",
    "Therefore, after finding the right point in parameter space for the deterministic model, the degree of stochasticity can be tuned by varying the product $\\lambda_0 V_{drip}$ such that:\n",
    "\n",
    "$\n",
    "\\quad \\tau = \\tau_{exp} = \\tau_{Poisson} \\\\\n",
    "\\quad r_0 = V_{drip} \\lambda_0 \\\\\n",
    "\\quad var_{t=0}[V(\\infty)] = \\sigma^2_V = V^2_{drip} \\lambda_0 \\tau \\\\\n",
    "\\quad \\Rightarrow V^2_{drip} \\left ( \\dfrac{r_0}{V_{drip}} \\right ) \\tau = \\sigma^2_V \\\\\n",
    "\\quad \\Rightarrow V_{drip} = \\dfrac{\\sigma^2_V}{r_0 \\tau} \\\\\n",
    "\\quad \\Rightarrow \\lambda_0 = \\dfrac{r^2_0 \\tau}{\\sigma^2_V}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "lam_drip_0 = 1.0 # initial drip parameter\n",
    "lam_lick = 1.0/5.0 # lick parameter\n",
    "lick_timeout = 1.0 # minimum time between rewarded licks\n",
    "tau = 30.0 # rate of decay of lambda\n",
    "V_drip = 2.0 # drip volume (uL)\n",
    "V_reward = 2.0 # reward volume (uL)\n",
    "t_0 = 0 # initial time (s)\n",
    "s = 300 # end time (s)\n",
    "num_patches = 1000\n",
    "\n",
    "# Poisson parameter functions for drip process\n",
    "lam_drip = lambda t: lam_drip_0*np.exp(-t/tau) # lambda(t)\n",
    "Lam_drip = lambda t, s: lam_drip_0*tau*(np.exp(-t/tau) - np.exp(-(t+s)/tau)) # integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Generate inter-reward intervals\n",
    "I think this is actually the wrong way to do it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial distribution\n",
    "mouse = Poisson(lam_lick)\n",
    "drip = Poisson(lam_drip)\n",
    "\n",
    "# Placeholders (for debugging)\n",
    "t_drip = [] # drip times\n",
    "t_lick = [] # lick times\n",
    "t_event = []\n",
    "V_bucket = [] # bucket volume\n",
    "\n",
    "# Initialize variables\n",
    "bucket = 0.0 # volume in bucket\n",
    "t, t_next_drip, t_next_lick = t_0, t_0, t_0\n",
    "t_prev_reward = -lick_timeout\n",
    "\n",
    "while (t <= t_0 + s):\n",
    "    # Increment timer to next event\n",
    "    t = min(t_next_drip, t_next_lick)\n",
    "    \n",
    "    # Update Poisson params\n",
    "    lam = lam_drip(t)\n",
    "    drip.lam = lambda t: lam\n",
    "    drip.Lam = lambda t, s: lam*s\n",
    "    \n",
    "    # Handle drip event\n",
    "    if (t == t_next_drip):\n",
    "        bucket += V_drip\n",
    "        t_drip.append(t)\n",
    "        t_next_drip = t + drip.times(t_max=100.0)\n",
    "    \n",
    "    # Handle lick event\n",
    "    if (t == t_next_lick):\n",
    "        if (bucket >= V_reward) and (t - t_prev_reward > lick_timeout):\n",
    "            bucket -= V_reward\n",
    "            t_prev_reward = t\n",
    "        t_lick.append(t)\n",
    "        t_next_lick = t + mouse.times(t_max=100.0)\n",
    "        \n",
    "    # Log bucket volume\n",
    "    t_event.append(t)\n",
    "    V_bucket.append(bucket)\n",
    "    \n",
    "# Create arrays\n",
    "t_drip = np.array(t_drip)\n",
    "t_lick = np.array(t_lick)\n",
    "t_event = np.array(t_event)\n",
    "V_bucket = np.array(V_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.plot(t_event, V_bucket, label='bucket volume')\n",
    "ymin = ax.get_ylim()[0] - 0.1*np.diff(ax.get_ylim())\n",
    "ymax = ax.get_ylim()[0]\n",
    "ax.vlines(t_drip, ymin=ymin, ymax=ymax, color='green', alpha=0.5, label='drips')\n",
    "ax.vlines(t_lick, ymin=ymin, ymax=ymax, color='black', alpha=0.5, label='licks')\n",
    "ax.set_xlabel('time (s)')\n",
    "ax.set_ylabel('volume (uL)')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distributions\n",
    "drip = Poisson(lam_drip)\n",
    "mouse = Poisson(lam_lick)\n",
    "\n",
    "# Placeholders\n",
    "t_drip_all = []\n",
    "t_lick_all = []\n",
    "t_event_all = []\n",
    "V_bucket_all = []\n",
    "\n",
    "for patch in range(num_patches):\n",
    "    # Initialize variables\n",
    "    bucket = 0.0 # volume in bucket\n",
    "    t, t_next_drip, t_next_lick = t_0, t_0, t_0\n",
    "    t_prev_reward = -lick_timeout\n",
    "\n",
    "    # Placeholders (for debugging)\n",
    "    t_drip = [] # drip times\n",
    "    t_lick = [] # lick times\n",
    "    t_event = []\n",
    "    V_bucket = [] # bucket volume\n",
    "\n",
    "    while (t <= t_0 + s):\n",
    "        # Update Poisson params\n",
    "        lam = lam_drip(t)\n",
    "        drip.lam = lambda t: lam\n",
    "        drip.Lam = lambda t, s: lam*s\n",
    "\n",
    "        # Handle drip event\n",
    "        if (t == t_next_drip):\n",
    "            bucket += V_drip\n",
    "            t_drip.append(t)\n",
    "            t_next_drip = t + drip.times(t_max=100.0)\n",
    "\n",
    "        # Handle lick event\n",
    "        if (t == t_next_lick):\n",
    "            if (bucket >= V_reward) and (t - t_prev_reward > lick_timeout):\n",
    "                bucket -= V_reward\n",
    "                t_prev_reward = t\n",
    "            t_lick.append(t)\n",
    "            t_next_lick = t + mouse.times(t_max=100.0)\n",
    "\n",
    "        # Log bucket volume\n",
    "        t_event.append(t)\n",
    "        V_bucket.append(bucket)\n",
    "        \n",
    "        # Increment timer to next event\n",
    "        t = min(t_next_drip, t_next_lick)\n",
    "    \n",
    "    # Log simulation\n",
    "    t_drip_all.append(np.array(t_drip))\n",
    "    t_lick_all.append(np.array(t_lick))\n",
    "    t_event_all.append(np.array(t_event))\n",
    "    V_bucket_all.append(np.array(V_bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check mean and variance of N(s)\n",
    "N_s = np.zeros([num_patches])\n",
    "for i in range(num_patches):\n",
    "    N_s[i] = len(t_drip_all[i])\n",
    "    \n",
    "# Print results of N(s)\n",
    "print('Number of drips from t=%.2f s to t=%.2f s:' % (t_0, s))\n",
    "print('\\tExpected mean: %.5f' % (Lam_drip(t_0, s)))\n",
    "print('\\tActual mean: %.5f' % (np.mean(N_s)))\n",
    "print('\\tExpected std: %.5f' % ((Lam_drip(t_0, s))**0.5))\n",
    "print('\\tActual std: %.5f' % (np.std(N_s)))\n",
    "\n",
    "# Interpolate for plotting purposes\n",
    "t_interp = np.linspace(t_0, t_0 + s, num=10*s)\n",
    "V_interp = np.zeros([num_patches, t_interp.size])\n",
    "for i in range(num_patches):\n",
    "    F = interp1d(t_event_all[i], V_bucket_all[i])\n",
    "    idx_max = np.argwhere(t_interp <= t_event_all[i][-1]).flatten()[-1]\n",
    "    V_interp[i, :idx_max+1] = F(t_interp[:idx_max+1])\n",
    "    V_interp[i, idx_max+1:] = F(t_interp[idx_max])\n",
    "\n",
    "# Plot results\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "mean = np.mean(V_interp, axis=0)\n",
    "std = np.std(V_interp, axis=0)\n",
    "ax.plot(t_interp, mean, color='C0', alpha=0.25, label='bucket volume')\n",
    "ax.fill_between(t_interp, y1=mean-std, y2=mean+std, color='C0', alpha=0.25)\n",
    "ax.set_xlabel('time (s)')\n",
    "ax.set_ylabel('volume (uL)')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Generate event times and prune\n",
    "The correct way to generate inhomogeneous Poisson processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distributions\n",
    "drip = Poisson(lam=lam_drip, Lam=Lam_drip, homogeneous=False)\n",
    "mouse = Poisson(lam_lick)\n",
    "\n",
    "# Get drip and lick events\n",
    "t_drip = drip.times(interevent=False, t=0, s=s, t_max=100)\n",
    "t_lick = mouse.times(interevent=False, t=0, s=s)\n",
    "\n",
    "# Placeholders (for debugging)\n",
    "t_event = []\n",
    "V_bucket = [] # bucket volume\n",
    "\n",
    "# There should be a vectorized way to do this...\n",
    "bucket = 0.0 # volume in bucket\n",
    "t = t_0\n",
    "idx_drip = 0\n",
    "idx_lick = 0\n",
    "t_prev_reward = -lick_timeout\n",
    "while (True): \n",
    "    # Determine event type\n",
    "    if (idx_drip < t_drip.size) and (idx_lick < t_lick.size):\n",
    "        is_drip = (t_drip[idx_drip] < t_lick[idx_lick])\n",
    "    elif (idx_drip < t_drip.size) and (idx_lick == t_lick.size):\n",
    "        is_drip = True\n",
    "    elif (idx_drip == t_drip.size) and (idx_lick < t_lick.size):\n",
    "        is_drip = False\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "    # Handle drip event\n",
    "    if is_drip:\n",
    "        t = t_drip[idx_drip]\n",
    "        bucket += V_drip\n",
    "        idx_drip += 1\n",
    "    \n",
    "    # Handle lick event\n",
    "    else:\n",
    "        t = t_lick[idx_lick]\n",
    "        if (bucket >= V_reward) and (t - t_prev_reward > lick_timeout):\n",
    "            bucket -= V_reward\n",
    "            t_prev_reward = t\n",
    "        idx_lick += 1\n",
    "        \n",
    "    # Log bucket volume\n",
    "    t_event.append(t)\n",
    "    V_bucket.append(bucket)\n",
    "    \n",
    "# Create arrays\n",
    "t_event = np.array(t_event)\n",
    "V_bucket = np.array(V_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.plot(t_event, V_bucket, label='bucket volume')\n",
    "ymin = ax.get_ylim()[0] - 0.1*np.diff(ax.get_ylim())\n",
    "ymax = ax.get_ylim()[0]\n",
    "ax.vlines(t_drip, ymin=ymin, ymax=ymax, color='green', alpha=0.5, label='drips')\n",
    "ax.vlines(t_lick, ymin=ymin, ymax=ymax, color='black', alpha=0.5, label='licks')\n",
    "ax.set_xlabel('time (s)')\n",
    "ax.set_ylabel('volume (uL)')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distributions\n",
    "drip = Poisson(lam=lam_drip, Lam=Lam_drip, homogeneous=False)\n",
    "mouse = Poisson(lam_lick)\n",
    "\n",
    "# Placeholders\n",
    "t_drip_all = []\n",
    "t_lick_all = []\n",
    "t_event_all = []\n",
    "V_bucket_all = []\n",
    "\n",
    "for patch in range(num_patches):\n",
    "    # Initialize variables\n",
    "    t = t_0 # current time (s)\n",
    "    bucket = 0.0 # volume in bucket\n",
    "\n",
    "    # Get drip and lick events\n",
    "    t_drip = drip.times(interevent=False, t=t_0, s=s, t_max=100)\n",
    "    t_lick = mouse.times(interevent=False, t=t_0, s=s)\n",
    "\n",
    "    # Placeholders (for debugging)\n",
    "    t_event = []\n",
    "    V_bucket = [] # bucket volume\n",
    "\n",
    "    # There should be a vectorized way to do this...\n",
    "    idx_drip = 0\n",
    "    idx_lick = 0\n",
    "    t_prev_reward = -lick_timeout\n",
    "    while (True): \n",
    "        # Determine event type\n",
    "        if (idx_drip < t_drip.size) and (idx_lick < t_lick.size):\n",
    "            is_drip = (t_drip[idx_drip] < t_lick[idx_lick])\n",
    "        elif (idx_drip < t_drip.size) and (idx_lick == t_lick.size):\n",
    "            is_drip = True\n",
    "        elif (idx_drip == t_drip.size) and (idx_lick < t_lick.size):\n",
    "            is_drip = False\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        # Handle drip event\n",
    "        if is_drip:\n",
    "            t = t_drip[idx_drip]\n",
    "            bucket += V_drip\n",
    "            idx_drip += 1\n",
    "\n",
    "        # Handle lick event\n",
    "        else:\n",
    "            t = t_lick[idx_lick]\n",
    "            if (bucket >= V_reward) and (t - t_prev_reward > lick_timeout):\n",
    "                bucket -= V_reward\n",
    "                t_prev_reward = t\n",
    "            idx_lick += 1\n",
    "\n",
    "        # Log bucket volume\n",
    "        t_event.append(t)\n",
    "        V_bucket.append(bucket)\n",
    "\n",
    "    # Log simulation\n",
    "    t_drip_all.append(t_drip)\n",
    "    t_lick_all.append(t_lick)\n",
    "    t_event_all.append(np.array(t_event))\n",
    "    V_bucket_all.append(np.array(V_bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check mean and variance of N(s)\n",
    "N_s = np.zeros([num_patches])\n",
    "for i in range(num_patches):\n",
    "    N_s[i] = len(t_drip_all[i])\n",
    "    \n",
    "# Print results of N(s)\n",
    "print('Number of drips from t=%.2f s to t=%.2f s:' % (t_0, s))\n",
    "print('\\tExpected mean: %.5f' % (Lam_drip(t_0, s)))\n",
    "print('\\tActual mean: %.5f' % (np.mean(N_s)))\n",
    "print('\\tExpected std: %.5f' % ((Lam_drip(t_0, s))**0.5))\n",
    "print('\\tActual std: %.5f' % (np.std(N_s)))\n",
    "\n",
    "# Interpolate for plotting purposes\n",
    "t_interp = np.linspace(t_0, t_0 + s, num=10*s)\n",
    "V_interp = np.zeros([num_patches, t_interp.size])\n",
    "for i in range(num_patches):\n",
    "    F = interp1d(np.insert(t_event_all[i], 0, 0.0), \n",
    "                 np.insert(V_bucket_all[i], 0, 0.0))\n",
    "    idx_max = np.argwhere(t_interp <= t_event_all[i][-1]).flatten()[-1]\n",
    "    V_interp[i, :idx_max+1] = F(t_interp[:idx_max+1])\n",
    "    V_interp[i, idx_max+1:] = F(t_interp[idx_max])\n",
    "\n",
    "# Plot results\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "mean = np.mean(V_interp, axis=0)\n",
    "std = np.std(V_interp, axis=0)\n",
    "ax.plot(t_interp, mean, color='C0', alpha=0.25, label='bucket volume')\n",
    "ax.fill_between(t_interp, y1=mean-std, y2=mean+std, color='C0', alpha=0.25)\n",
    "ax.set_xlabel('time (s)')\n",
    "ax.set_ylabel('volume (uL)')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks right. In the first method, the mean is too long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation of waiting time\n",
    "For a non-homogeneous Poisson process, what is the expected waiting time to the first event (after some time $t$)? For derivations of the probability distribution functions and expected wait times, see *Expected Reward Times* in *Appendix*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability distribution functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "lam0 = 2.5 # initial Poisson rate\n",
    "tau = 6.0 # decay rate\n",
    "lam = lambda t: lam0*np.exp(-t/tau)\n",
    "Lam = lambda t, s: lam0*tau*(np.exp(-t/tau) - np.exp(-(t+s)/tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define probability functions for nth event occuring within [t, t+s]\n",
    "P = lambda t, s, n: np.exp(-Lam(t,s))*(Lam(t,s)**n)/factorial(n)\n",
    "p = lambda t, s, n: (1.0/factorial(n))*np.exp(-Lam(t,s))*lam(t+s)*(Lam(t,s)**(n-1))*(Lam(t,s) - n)\n",
    "\n",
    "# Define probability functions for nth event occuring at time t\n",
    "F = lambda t, s, N: 1.0 - np.sum(np.exp(-Lam(t, s))*np.power(Lam(t, s), np.arange(N))/factorial(np.arange(N)), axis=1)\n",
    "f = lambda t, s, N: np.sum((1.0/factorial(np.arange(N)))*lam(t+s)*np.power(Lam(t, s), np.arange(N)-1)\n",
    "                           *np.exp(-Lam(t, s))*(Lam(t, s) - np.arange(N)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot probability distribution\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "cmap = plt.get_cmap('bone')\n",
    "t0 = 0.0\n",
    "s = np.linspace(0.1, 15.0, num=1000).reshape([-1, 1])\n",
    "\n",
    "# Plot probability density function\n",
    "N = 10\n",
    "for n in range(N):\n",
    "    ax[0].plot(t0 + s,\n",
    "               p(t0, s, n),\n",
    "               color=cmap(0.5*n/N),\n",
    "               linestyle='--',\n",
    "               alpha=0.7)\n",
    "ax[0].plot(t0 + s,\n",
    "           f(t0, s, N),\n",
    "           color=cmap(0.0),\n",
    "           linestyle='-')\n",
    "ax[0].set_xlabel('time (s)')\n",
    "ax[0].set_ylabel('density')\n",
    "ax[0].set_title('probability density functions')\n",
    "\n",
    "# Plot cumulative distribution function\n",
    "N = 10\n",
    "for n in range(N):\n",
    "    ax[1].plot(t0 + s,\n",
    "               P(t0, s, n),\n",
    "               color=cmap(0.5*n/N),\n",
    "               linestyle='--',\n",
    "               alpha=0.7)\n",
    "ax[1].plot(t0 + s,\n",
    "           F(t0, s, N),\n",
    "           color=cmap(0.0),\n",
    "           linestyle='-')\n",
    "ax[1].set_xlabel('time (s)')\n",
    "ax[1].set_ylabel('probability')\n",
    "ax[1].set_title('cumulative distribution functions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the probability density functions essentially cancel out the peaks of the *pdf*s for prior events, pushing the peak for the $n^{th}$ event outward in time. Notice also how the *cdf* may not approach unity even in the limit that $t \\rightarrow \\infty$. Because the Poisson rate is exponentially decaying in time, there can be event sequences that never reach the $N^{th}$ event; for instance, the expected value of number of observations after some time $t$, $\\Lambda(t, \\infty)$ may be less than the number of observations need to reach event $N$. Thus, the value of $F(\\infty)$ represents the probability that the sequence will have an $N^{th}$ event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "t0 = np.array([[0.0]])\n",
    "s = np.linspace(0.01, 25.0, num=1000).reshape([-1, 1])\n",
    "N = 10\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15, 10))\n",
    "cmap = plt.get_cmap('bone')\n",
    "\n",
    "for n in np.arange(N+1):\n",
    "    # Plot unnormalized probability density\n",
    "    ax[0, 0].plot(t0 + s, \n",
    "                  f(t0, s, n),\n",
    "                  color=cmap(0.5*n/N),\n",
    "                  alpha=F(t0, 1000.0, n),\n",
    "                  label='n={:02d}'.format(n))\n",
    "    ax[0, 0].set_title('unnormalized pdf')\n",
    "    ax[0, 0].set_ylabel('density')\n",
    "    \n",
    "    # Plot normalized probability density\n",
    "    ax[1, 0].plot(t0 + s, \n",
    "                  f(t0, s, n)/F(t0, 1000.0, n),\n",
    "                  color=cmap(0.5*n/N),\n",
    "                  alpha=F(t0, 1000.0, n),\n",
    "                  label='n={:02d}'.format(n))\n",
    "    ax[1, 0].set_title('normalized pdf')\n",
    "    ax[1, 0].set_ylabel('density')\n",
    "    \n",
    "    # Plot unnormalized cumulative probability\n",
    "    ax[0, 1].plot(t0 + s, \n",
    "                  F(t0, s, n),\n",
    "                  color=cmap(0.5*n/N),\n",
    "                  alpha=F(t0, 1000.0, n),\n",
    "                  label='n={:02d}'.format(n))\n",
    "    ax[0, 1].set_title('unnormalized cdf')\n",
    "    ax[0, 1].set_ylabel('probability')\n",
    "    \n",
    "    # Plot normalized cumulative probability\n",
    "    ax[1, 1].plot(t0 + s, \n",
    "                  F(t0, s, n)/F(t0, 1000.0, n),\n",
    "                  color=cmap(0.5*n/N),\n",
    "                  alpha=F(t0, 1000.0, n),\n",
    "                  label='n={:02d}'.format(n))\n",
    "    ax[1, 1].set_title('normalized cdf')\n",
    "    ax[1, 1].set_ylabel('probability')\n",
    "    \n",
    "    \n",
    "\n",
    "# Label axes\n",
    "for ax_ in ax.reshape(-1):\n",
    "    ax_.set_xlabel('time (s)')\n",
    "    ax_.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings\n",
    "lam0 = 2.5 # initial Poisson rate\n",
    "tau = 6.0 # decay rate\n",
    "lam = lambda t: lam0*np.exp(-t/tau)\n",
    "Lam = lambda t, s: lam0*tau*(np.exp(-t/tau) - np.exp(-(t+s)/tau))\n",
    "model = Poisson(lam=lam, Lam=Lam, homogeneous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate events\n",
    "t0 = 1.0 # start time\n",
    "N = 1000 # number of simulations\n",
    "t = np.zeros([N])\n",
    "for n in range(N):\n",
    "    t[n] = model.times(interevent=False, t=t0, s=1000.0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate expected waiting time. Note that we have to reverse the order of\n",
    "# arguments since we're integrating over s, not t. Also, we need to pass the points\n",
    "# parameters since the integrand is close to zero for most of t > 0.0:\n",
    "# see https://stackoverflow.com/a/51053180\n",
    "pts = np.geomspace(0.01, 1000.0, num=25)\n",
    "g = lambda s, t: np.exp(-lam0*tau*(np.exp(-t/tau) - np.exp(-(t+s)/tau)))\n",
    "f = lambda s, t: s*lam(t+s)*g(s, t)\n",
    "t_est = integrate.quad(f, 0.0, 1000.0, args=(t0,), points=pts)[0]\n",
    "\n",
    "# Print results\n",
    "print('Expected mean time to first event: {:.3f}'.format(t_est))\n",
    "print('Actual mean time to first event:  {:.3f}'.format(t.mean() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure settings\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "cmap = plt.get_cmap('bone')\n",
    "\n",
    "# Plot histogram of simulated values\n",
    "ax.hist(t - t0, \n",
    "        color=cmap(0.4), \n",
    "        alpha=0.5,\n",
    "        bins=50,\n",
    "        density=True)\n",
    "ax.axvline(t.mean() - t0, \n",
    "           color=cmap(0.0), \n",
    "           linestyle='-',\n",
    "           alpha=0.5,\n",
    "           label='mean (simulated)')\n",
    "\n",
    "# Plot theoretical mean\n",
    "ax.axvline(t_est, \n",
    "           color=cmap(0.2), \n",
    "           linestyle='--',\n",
    "           alpha=0.5,\n",
    "           label='mean (theoretical)')\n",
    "\n",
    "# Label axes\n",
    "ax.set_xlabel('time to first event (s)')\n",
    "ax.set_ylabel('density')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nth event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings\n",
    "M = 10 # mth event to estimate waiting time\n",
    "lam0 = 2.5 # initial Poisson rate\n",
    "tau = 6.0 # decay rate\n",
    "lam = lambda t: lam0*np.exp(-t/tau)\n",
    "Lam = lambda t, s: lam0*tau*(np.exp(-t/tau) - np.exp(-(t+s)/tau))\n",
    "model = Poisson(lam=lam, Lam=Lam, homogeneous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate events\n",
    "t0 = 3.0 # start time\n",
    "N = 1000 # number of simulations\n",
    "t = np.ones([N])*np.nan\n",
    "for n in range(N):\n",
    "    try:\n",
    "        t[n] = model.times(interevent=False, t=t0, s=1000.0)[M-1]\n",
    "    except IndexError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine probability of Mth event occurring in sequence\n",
    "Lam_inf = lambda t: lam0*tau*np.exp(-t/tau) # Lam(t, inf)\n",
    "F0 = lambda t, n: 1.0 - np.exp(-Lam_inf(t))*np.sum(np.power(Lam_inf(t), np.arange(n))\n",
    "                                                   /factorial(np.arange(n)))\n",
    "\n",
    "# Method 1: estimate by dividing rate by M\n",
    "pts = np.geomspace(0.01, 1000.0, num=25)\n",
    "g = lambda s, t: np.exp(-lam0/M*tau*(np.exp(-t/tau) - np.exp(-(t+s)/tau)))\n",
    "h = lambda s, t: s*(lam(t+s)/M)*g(s, t)\n",
    "t_est_1 = integrate.quad(h, 0.0, 1000.0, args=(t0,), points=pts)[0]\n",
    "t_est_1 /= F0(t0, M)\n",
    "\n",
    "# Method 2: compute the integral in two parts\n",
    "# 1) N(t) = 0 component\n",
    "# 2) N(t) = 1,...,N-1 component\n",
    "h = lambda s, t, m: s*lam(t+s)*(Lam(t, s)**(m-1))*np.exp(-Lam(t, s))*(Lam(t, s) - m)\n",
    "t_est_2 = 0.0\n",
    "for m in range(M):\n",
    "    t_est_2 += (1.0/factorial(m))*integrate.quad(h, 0.0, 1000.0, \n",
    "                                                 args=(t0, m), \n",
    "                                                 points=pts)[0]\n",
    "t_est_2 /= F0(t0, M)\n",
    "\n",
    "# Method 3: leave sum inside integral (for faster computation)\n",
    "h = lambda s, t, m: s*np.sum((1.0/factorial(np.arange(m)))*lam(t+s)*np.power(Lam(t, s), np.arange(m)-1)\n",
    "                             *np.exp(-Lam(t, s))*(Lam(t, s) - np.arange(m)))\n",
    "t_est_3 = integrate.quad(h, 0.0, 1000.0, \n",
    "                         args=(t0, M), \n",
    "                         points=pts)[0]\n",
    "t_est_3 /= F0(t0, M)\n",
    "\n",
    "# Print results\n",
    "print('Expected probability of observing Mth event: {:.3f}'.format(F0(t0, M)))\n",
    "print('Actual probability of observing Mth event:   {:.3f}'.format(np.sum(~np.isnan(t))/t.shape[0]))\n",
    "print()\n",
    "print('Expected mean time to mth event (method 1): {:.3f}'.format(t_est_1))\n",
    "print('Expected mean time to mth event (method 2): {:.3f}'.format(t_est_2))\n",
    "print('Expected mean time to mth event (method 3): {:.3f}'.format(t_est_3))\n",
    "print('Actual mean time to mth event:              {:.3f}'.format(np.nanmean(t) - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure settings\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "cmap = plt.get_cmap('bone')\n",
    "\n",
    "# Plot histogram of simulated values\n",
    "ax.hist(t - t0, \n",
    "        color=cmap(0.25), \n",
    "        alpha=0.5,\n",
    "        bins=50,\n",
    "        density=True)\n",
    "ax.axvline(np.nanmean(t) - t0, \n",
    "           color=cmap(0.2), \n",
    "           linestyle='-',\n",
    "           alpha=0.5,\n",
    "           label='mean (simulated)')\n",
    "\n",
    "# Plot theoretical means\n",
    "ax.axvline(t_est_1, \n",
    "           color=cmap(0.0), \n",
    "           linestyle='--',\n",
    "           alpha=0.5,\n",
    "           label='mean (method 1)')\n",
    "ax.axvline(t_est_2, \n",
    "           color=cmap(0.0), \n",
    "           linestyle=':',\n",
    "           alpha=0.5,\n",
    "           label='mean (method 2/3)')\n",
    "\n",
    "# Label axes\n",
    "ax.set_xlabel('time to mth event (s)')\n",
    "ax.set_ylabel('density')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consecutive events\n",
    "Is the summation of expected waiting times for consecutive events the same as the expected waiting time for the last event?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings\n",
    "M = 10 # mth event to estimate waiting time\n",
    "K = 10 # number of mth events to estimate\n",
    "lam0 = 2.5*M # initial Poisson rate\n",
    "tau = 6.0 # decay rate\n",
    "lam = lambda t: lam0*np.exp(-t/tau)\n",
    "Lam = lambda t, s: lam0*tau*(np.exp(-t/tau) - np.exp(-(t+s)/tau))\n",
    "model = Poisson(lam=lam, Lam=Lam, homogeneous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate events\n",
    "t0 = 2.0 # start time\n",
    "N = 1000 # number of simulations\n",
    "t = np.ones([N, K])*np.nan\n",
    "for n in range(N):\n",
    "    try:\n",
    "        t[n] = model.times(interevent=False, t=t0, s=1000.0)[np.array([(k+1)*M-1 for k in range(K)])]\n",
    "    except IndexError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine probability of Mth event occurring in sequence\n",
    "Lam_inf = lambda t: lam0*tau*np.exp(-t/tau) # Lam(t, inf)\n",
    "F0 = lambda t, n: 1.0 - np.exp(-Lam_inf(t))*np.sum(np.power(Lam_inf(t), np.arange(n))\n",
    "                                                   /factorial(np.arange(n)))\n",
    "\n",
    "# Method 1: estimate each event time independently\n",
    "pts = np.geomspace(0.01, 1000.0, num=25)\n",
    "h = lambda s, t, m: s*np.sum((1.0/factorial(np.arange(m)))*lam(t+s)*np.power(Lam(t, s), np.arange(m)-1)\n",
    "                             *np.exp(-Lam(t, s))*(Lam(t, s) - np.arange(m)))\n",
    "t_est_1 = np.zeros([K])\n",
    "for k in range(K):\n",
    "    t_est_1[k] = integrate.quad(h, 0.0, 100.0, \n",
    "                                args=(t0, (k+1)*M), \n",
    "                                points=pts)[0]\n",
    "    t_est_1[k] /= F0(t0, (k+1)*M)\n",
    "\n",
    "# Method 2: estimate each event time from previous estimate\n",
    "t_est_2 = np.zeros([K])\n",
    "t_start = t0\n",
    "for k in range(K):\n",
    "    t_est_2[k] = integrate.quad(h, 0.0, 100.0, \n",
    "                                args=(t_start, M), \n",
    "                                points=pts)[0]\n",
    "    t_est_2[k] /= F0(t0, M)\n",
    "    t_start += t_est_2[k]\n",
    "t_est_2 = np.cumsum(t_est_2)\n",
    "\n",
    "# Print results\n",
    "print('Expected probability of observing K*Mth event: {:.3f}'.format(F0(t0, M*K)))\n",
    "print('Actual probability of observing K*Mth event:   {:.3f}'.format(np.sum(~np.isnan(t[:,-1]))/t.shape[0]))\n",
    "print()\n",
    "print('Expected mean time to events (method 1): {}'.format(', '.join(['{:.3f}'.format(tk) for tk in t_est_1])))\n",
    "print('Expected mean time to events (method 2): {}'.format(', '.join(['{:.3f}'.format(tk) for tk in t_est_2])))\n",
    "print('Actual mean time to events:              {}'.format(', '.join(['{:.3f}'.format(tk - t0) for tk in np.nanmean(t, axis=0)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unobserved drips\n",
    "If we know the animal did not observe a reward within some time period (e.g. between the last observed reward and patch leaving), how can we leverage that information to better predict future reward times? Two naive approaches would both miss this important information:\n",
    "1. Use the patch leaving time as a starting point. In this case, we would assume no unobserved Poisson drips have occurred since the last reward, which is unlikely to occur and leads to an overestimation of the next reward time.\n",
    "2. Use the last reward time as a starting point. Although we can be sure that no unobserved drips were present at the starting point, this may lead to predictions that the next reward time occurs prior to the known patch leaving time, which fails to incorporate the observation that no observed rewards occurred during that time period.\n",
    "\n",
    "Instead, we can estimate the number of hidden Poisson drips that have occurred given that the number must add up to be less than the reward droplet size. For a derivation of the equations below, see *Expected number of unobserved events* in the *Appendix*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings\n",
    "lam0 = 12.5 # initial Poisson rate\n",
    "tau = 6.0 # decay rate\n",
    "lam = lambda t: lam0*np.exp(-t/tau)\n",
    "Lam = lambda t, s: lam0*tau*(np.exp(-t/tau) - np.exp(-(t+s)/tau))\n",
    "model = Poisson(lam=lam, Lam=Lam, homogeneous=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define probability, marginal probability, and expectation functions\n",
    "p  = lambda s, t, n: np.exp(-Lam(t,s))*(Lam(t,s)**n)/factorial(n)\n",
    "p0 = lambda s, t, L: np.sum(p(s, t, np.arange(L)))\n",
    "E  = lambda s, t, L: np.sum(np.arange(L)*p(s, t, np.arange(L)))/p0(s, t, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "N = 10 # number of hidden drips per observation\n",
    "t = 10.0 # initial time\n",
    "s = np.linspace(0.0, 10.0, 100) # time without observation\n",
    "\n",
    "# Plot results\n",
    "fig, ax = plt.subplots()\n",
    "cmap = plt.get_cmap('bone')\n",
    "ax.plot(t+s,\n",
    "        Lam(t, s),\n",
    "        color=cmap(0.20),\n",
    "        label='without prior')\n",
    "ax.plot(t+s,\n",
    "        np.array([E(x, t, N) for x in s]),\n",
    "        color=cmap(0.70),\n",
    "        label='with prior')\n",
    "ax2 = ax.twinx()\n",
    "for n in range(N):\n",
    "    ax2.plot(t+s, \n",
    "             np.array([p(x, t, n)/p0(x, t, N) for x in s]),\n",
    "             color=cmap(n/N),\n",
    "             linestyle='--',\n",
    "             alpha=0.75)\n",
    "    \n",
    "# Label axes\n",
    "ax.set_xlabel('time (s)', fontsize=14.0)\n",
    "ax.set_ylabel('E[n]', fontsize=14.0)\n",
    "ax2.set_ylabel('p(n)', fontsize=14.0)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homogeneous Poisson process\n",
    "The mean and variance of a homogeneous Poisson process (that is, $\\lambda(t) = k \\: \\forall \\: t$) are given by:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\quad \\mathbb{E}[N(s)] \n",
    "&= \\sum_{n=0}^{\\infty} n \\cdot P(N(s) = n) \\\\\n",
    "&= 0 + \\sum_{n=1}^{\\infty} n \\cdot P(N(s) = n) \\\\\n",
    "&= \\sum_{n=1}^{\\infty} n e^{-\\lambda s} \\frac{(\\lambda s)^n}{n!} \\\\\n",
    "&= \\sum_{n=1}^{\\infty} e^{-\\lambda s} \\frac{(\\lambda s)^n}{(n-1)!} \\\\\n",
    "&= (\\lambda s) (e^{-\\lambda s}) \\sum_{n=1}^{\\infty} \\frac{(\\lambda s)^{n-1}}{(n-1)!} \\\\\n",
    "&= (\\lambda s) (e^{-\\lambda s}) (e^{\\lambda s}) \\\\\n",
    "&= \\lambda s\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\quad \\mathbb{E}[\\left ( N(s) \\right ) \\left ( N(s) - 1 \\right )]\n",
    "&= \\sum_{n=0}^{\\infty} n(n-1) \\cdot P(N(s) = n) \\\\\n",
    "&= 0 + 0 + \\sum_{n=2}^{\\infty} n(n-1) e^{-\\lambda s} \\frac{(\\lambda s)^n}{n!} \\\\\n",
    "&= \\sum_{n=2}^{\\infty} e^{-\\lambda s} \\frac{(\\lambda s)^n}{(n-2)!} \\\\\n",
    "&= (\\lambda s)^2 (e^{-\\lambda s}) \\sum_{n=2}^{\\infty} \\frac{(\\lambda s)^{n-2}}{(n-2)!} \\\\\n",
    "&= (\\lambda s)^2 (e^{-\\lambda s}) (e^{\\lambda s}) \\\\\n",
    "&= (\\lambda s)^2\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\quad var[N(s)]\n",
    "&= \\mathbb{E}[(N(s))^2] - \\left ( \\mathbb{E}[N(s)] \\right )^2 \\\\\n",
    "&= \\mathbb{E}[(N(s))^2] - \\mathbb{E}[N(s)] + \\mathbb{E}[N(s)] - \\left ( \\mathbb{E}[N(s)] \\right )^2 \\\\\n",
    "&= \\mathbb{E}[(N(s))^2 - N(s)] + \\mathbb{E}[N(s)] - \\left ( \\mathbb{E}[N(s)] \\right )^2 \\\\\n",
    "&= \\mathbb{E}[(N(s))(N(s) - 1)] + \\mathbb{E}[N(s)] - \\left ( \\mathbb{E}[N(s)] \\right )^2 \\\\\n",
    "&= (\\lambda s)^2 + \\lambda s - (\\lambda s)^2 \\\\\n",
    "&= \\lambda s\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "where we used the Taylor expansion $e^x = \\sum_{n=0}^{\\infty} \\frac{x^n}{n!}$ in the infinite sums."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-homogeneous Poisson process\n",
    "\n",
    "When I try to apply this to a non-homogeneous process with parameter $\\lambda(t)$ that is dependent on $t$, I get the mean:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\quad \\mathbb{E}[N(s)] \n",
    "&= \\lim_{\\delta r \\to 0} \\sum_{r=0}^s \\left ( \\sum_{n=0}^{\\infty} n \\cdot P(N_r(\\delta r) = n) \\right ) \\\\\n",
    "&= \\int_0^s \\left ( \\sum_{n=0}^{\\infty} n \\cdot P(N_r(\\delta r) = n) \\right ) \\\\\n",
    "&= \\int_0^s \\left ( \\sum_{n=1}^{\\infty} n e^{-\\lambda(r) dr} \\frac{(\\lambda(r) dr)^n}{n!} \\right ) \\\\\n",
    "&= \\int_0^s \\left ( \\lambda(r) dr \\sum_{n=1}^{\\infty} e^{-\\lambda(r) dr} \\frac{(\\lambda(r) dr)^{n-1}}{(n-1)!} \\right ) \\\\\n",
    "&= \\int_0^s \\left ( \\lambda(r) dr \\right ) \\left ( e^{-\\lambda(r) dr} \\right ) \\left ( e^{\\lambda(r) dr} \\right ) \\\\\n",
    "&= \\int_0^s \\lambda(r) dr \\\\\n",
    "&= \\Lambda(t, s)\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "But when I try to derive the $\\mathbb{E}[\\left ( N(s) \\right ) \\left ( N(s) - 1 \\right )]$ term, I get stuck:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\quad \\mathbb{E}[\\left ( N(s) \\right ) \\left ( N(s) - 1 \\right )]\n",
    "&= \\int_0^s \\left ( \\sum_{n=0}^{\\infty} n(n-1) \\cdot P(N_r(dr) = n) \\right ) \\\\\n",
    "&= \\int_0^s \\left ( \\sum_{n=2}^{\\infty} n(n-1) e^{-\\lambda(r) dr} \\frac{(\\lambda(r) dr)^n}{n!} \\right ) \\\\\n",
    "&= \\int_0^s \\left ( \\left ( \\lambda(r) dr \\right )^2 e^{-\\lambda(r) dr} \\sum_{n=2}^{\\infty} \\frac{(\\lambda(r) dr)^{n-2}}{(n-2)!} \\right ) \\\\\n",
    "&= \\int_0^s \\left ( \\left ( \\lambda(r) dr \\right )^2 \\left ( e^{-\\lambda(r) dr}  \\right ) \\left ( e^{-\\lambda(r) dr} \\right ) \\right ) \\\\\n",
    "&= \\int_0^s \\left ( \\lambda(r) dr \\right )^2 \\\\\n",
    "&=^? \\left ( \\int_0^s \\lambda(r) dr \\right )^2\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Edit: [This answer](https://math.stackexchange.com/a/3293876/695405) to a similar question may be relevant.\n",
    "\n",
    "Edit: This has been answered on [StackExchange](https://math.stackexchange.com/a/3501155/695405)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance of a scaled Poisson process\n",
    "If we scale a (non-homogeneous) Poisson process by some constant $k$, such that we track a variable $M_t(s) = k N_t(s)$, then the mean and variance should be scaled by $k$ and $k^2$, respectively:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\quad \\mathbb{E}_t[M(s)] \n",
    "&= \\mathbb{E}_t[k N(s)] \\\\\n",
    "&= k \\mathbb{E}_t[N(s)] \\\\\n",
    "&= k \\Lambda(t, s) \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\quad var_t[M(s)] \n",
    "&= var_t[k N(s)] \\\\\n",
    "&= \\mathbb{E}_t[(k N(s))^2] - \\left ( \\mathbb{E}_t[k N(s)] \\right )^2 \\\\\n",
    "&= \\mathbb{E}_t[(k N(s))^2] - \\mathbb{E}_t[k^2 N(s)] + \\mathbb{E}_t[k^2 N(s)] - \\left ( \\mathbb{E}_t[k N(s)] \\right )^2 \\\\\n",
    "&= \\mathbb{E}_t[(k N(s))^2 - k^2 N(s)] + \\mathbb{E}_t[k^2 N(s)] - \\left ( \\mathbb{E}_t[k N(s)] \\right )^2 \\\\\n",
    "&= k^2 \\mathbb{E}_t[N(s) (N(s)-1)] + k^2 \\mathbb{E}_t[N(s)] - k^2 \\left ( \\mathbb{E}_t[N(s)] \\right )^2 \\\\\n",
    "&= k^2 \\left ( \\Lambda(t, s) \\right )^2 + k^2 \\Lambda(t, s) - k^2 \\left ( \\Lambda(t, s) \\right )^2 \\quad \\text{(given proof above)} \\\\\n",
    "&= k^2 \\Lambda(t, s)\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected reward times\n",
    "Given a time $t$, what are the expected times for rewards $\\{T_1, \\ldots, T_N\\}$ in the future? To start, let's calculate the cumulative distribution function (cdf) for the time of the $N^{th}$ event given that the current time is $t$. We will use the transformation $S_N = T_N - t$ and $s = t' - t$ for ease of calculation.\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\quad F_{S_N}(s; t) \n",
    "&= P(S_N \\leq s; t) \\\\\n",
    "&= 1 - P(S_n > s; t) \\\\\n",
    "&= 1 - \\sum_{n=0}^{N-1} P(N(s) = n; t) \\\\\n",
    "&= 1 - \\sum_{n=0}^{N-1} \\left ( e^{-\\Lambda(t,s)} \\left ( \\dfrac{\\Lambda(t,s)^n}{n!} \\right ) \\right )\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "where\n",
    "\n",
    "$\n",
    "\\quad \\Lambda(t,s) = \\int_{t}^{t+s} \\lambda(s')ds'\n",
    "$\n",
    "\n",
    "Next, we compute the probability density function (pdf) by taking the derivative of the cdf:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\quad f_{S_N}(s; t)\n",
    "&= \\dfrac{d F_{S_N}(s; t)}{ds} \\\\\n",
    "&= \\dfrac{d}{ds} \\left ( 1 - \\sum_{n=0}^{N-1} e^{-\\Lambda(t,s)} \\dfrac{\\Lambda(t,s)^n}{n!} \\right ) \\\\\n",
    "&= -\\dfrac{d}{ds} \\left ( \\sum_{n=0}^{N-1} e^{-\\Lambda(t,s)} \\dfrac{\\Lambda(t,s)^n}{n!} \\right ) \\\\\n",
    "&= -\\sum_{n=0}^{N-1} \\dfrac{d}{ds} \\left ( e^{-\\Lambda(t,s)} \\dfrac{\\Lambda(t,s)^n}{n!} \\right ) \\\\\n",
    "&= -\\sum_{n=0}^{N-1} \\dfrac{1}{n!} \\dfrac{d}{ds} \\left ( e^{-\\Lambda(t,s)} \\Lambda(t,s)^n \\right ) \\\\\n",
    "&= -\\sum_{n=0}^{N-1} \\dfrac{1}{n!} \\Big [ \\left ( -\\lambda(t+s) e^{-\\Lambda(t,s)} \\right ) \\Lambda(t,s)^n\n",
    "+ e^{-\\Lambda(t,s)} \\left ( n \\Lambda(t,s)^{n-1} \\lambda(t+s) \\right ) \\Big ] \\\\\n",
    "&= \\sum_{n=0}^{N-1} \\dfrac{1}{n!} \\Big [ e^{-\\Lambda(t,s)} \\lambda(t+s) \\Lambda(t,s)^{n-1}\n",
    "\\left ( \\Lambda(t,s) - n \\right ) \\Big ] \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "where\n",
    "\n",
    "$\n",
    "\\quad \\dfrac{d \\Lambda(t,s)}{ds} \n",
    "= \\dfrac{d}{ds} \\left ( \\int_{t}^{t+s} \\lambda(s')ds' \\right )\n",
    "= \\lambda(t+s)\n",
    "$\n",
    "\n",
    "by the Fundamental Theorem of Calculus (note the importance of the limit in the definite integral). When we check the criteria for a valid probability density function (or cumulative distribution function), however, we run into a problem: $\\lim_{s \\rightarrow \\infty} (F_{S_N}(s; t)) < 1$, or equivalently $\\int_t^\\infty f_{S_N}(s'; t) ds' < 1$. We can see that this is the case because:\n",
    "\n",
    "$\\begin{align}\n",
    "\\quad \\lim_{s \\rightarrow \\infty} (F_{S_N}(s; t))\n",
    "&= \\lim_{s \\rightarrow \\infty} \\left ( 1 - \\sum_{n=0}^{N-1} e^{-\\Lambda(t,s)} \\dfrac{\\Lambda(t,s)^n}{n!} \\right ) \\\\\n",
    "&= 1 - \\lim_{s \\rightarrow \\infty} \\left ( \\sum_{n=0}^{N-1} e^{-\\Lambda(t,s)} \\dfrac{\\Lambda(t,s)^n}{n!} \\right ) \\\\\n",
    "&= 1 - \\sum_{n=0}^{N-1} \\dfrac{1}{n!} \\lim_{s \\rightarrow \\infty} \\left ( e^{-\\Lambda(t,s)} \\Lambda(t,s)^n \\right ) \\\\\n",
    "&= 1 - \\sum_{n=0}^{N-1} \\dfrac{1}{n!} \\left ( e^{-a} a^n \\right )\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "where \n",
    "\n",
    "$\\begin{align}\n",
    "\\quad a \n",
    "&= \\Lambda(t,\\infty) \\\\\n",
    "&= \\int_{t}^{\\infty} \\lambda(s')ds' \\\\\n",
    "&= -\\lambda_0 \\tau \\left ( e^{-\\frac{s}{\\tau}} \\right ) \\bigg |^{\\infty}_{t} \\\\\n",
    "&= -\\lambda_0 \\tau \\left ( 0 - e^{-\\frac{t}{\\tau}} \\right ) \\\\\n",
    "&= \\lambda_0 \\tau e^{-\\frac{t}{\\tau}}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "We can rewrite the summation term as:\n",
    "\n",
    "$\\begin{align}\n",
    "\\quad \\sum_{n=0}^{N-1} \\dfrac{1}{n!} \\left ( e^{-a} a^n \\right )\n",
    "&= e^{-a} \\sum_{n=0}^{N-1} \\dfrac{a^n}{n!} \\\\\n",
    "&= e^{-a} \\left ( \\sum_{n=0}^{\\infty} \\dfrac{a^n}{n!} - \\sum_{n=N}^{\\infty} \\dfrac{a^n}{n!} \\right ) \\\\\n",
    "&= e^{-a} \\left ( e^a - \\sum_{n=N}^{\\infty} \\dfrac{a^n}{n!} \\right ) \\\\\n",
    "&= 1 - \\dfrac{\\sum_{n=N}^{\\infty} \\dfrac{a^n}{n!}}{e^{a}}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Plugging this back into our original equation:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\quad \\lim_{s \\rightarrow \\infty} (F_{S_N}(s; t))\n",
    "&= 1 - \\sum_{n=0}^{N-1} \\dfrac{1}{n!} \\left ( e^{-a} a^n \\right ) \\\\\n",
    "&= 1 - \\left ( 1 - \\dfrac{\\sum_{n=N}^{\\infty} \\dfrac{a^n}{n!}}{e^{a}} \\right ) \\\\\n",
    "&= \\dfrac{\\sum_{n=N}^{\\infty} \\dfrac{a^n}{n!}}{e^{a}}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "and observing the following inequality leads us to:\n",
    "\n",
    "$\n",
    "\\quad \\sum_{n=N}^{\\infty} \\dfrac{a^n}{n!} < \\sum_{n=0}^{\\infty} \\dfrac{a^n}{n!} \\\\\n",
    "\\quad \\Rightarrow \\sum_{n=N}^{\\infty} \\dfrac{a^n}{n!} < e^a \\\\\n",
    "\\quad \\Rightarrow \\dfrac{\\sum_{n=N}^{\\infty} \\dfrac{a^n}{n!}}{e^a} < 1 \\\\\n",
    "\\quad \\Rightarrow \\lim_{s \\rightarrow \\infty} (F_{S_N}(s; t)) < 1\n",
    "$\n",
    "\n",
    "Upon further thought, the inequality makes sense: because $\\lim_{t \\rightarrow \\infty} \\lambda(t) = 0$ and $\\Lambda(t, \\infty)$ is finite, individual sequences may never reach the $N^{th}$ event, particularly for large $N$ and/or large $t$. We can thus think of the cumulative distribution function as a factor of two components: 1) the probability of ever seeing the $N^{th}$ event for a given sequence, and 2) the probability of when that event would occur if it is observed.\n",
    "\n",
    "$\n",
    "\\quad F_{S_N}(s; t)\n",
    "= P(S_N < s; t) \\dfrac{P(S_N < \\infty; t)}{P(S_N < \\infty; t)}\n",
    "= \\dfrac{P(S_N < s; t)}{P(S_N < \\infty; t)} P(S_N < \\infty; t)\n",
    "= \\tilde{F}_{S_N}(s; t) F_0\n",
    "$\n",
    "\n",
    "where \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\quad F_0 \n",
    "&= P(S_N < \\infty; t) \\\\\n",
    "&= 1 - \\sum_{n=0}^{N-1} P(N(\\infty) = n; t) \\\\\n",
    "&= 1 - \\sum_{n=0}^{N-1} e^{-\\Lambda(t,\\infty)} \\dfrac{\\Lambda(t,\\infty)^n}{n!} \\\\\n",
    "&= 1 - \\sum_{n=0}^{N-1} e^{-a} \\dfrac{a^n}{n!}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "represents the probability that the $N^{th}$ even is observed in a given sequence ($a$ has been calculated previously; see above). We thus denote $\\tilde{F}$ as the normalized cumulative distribution function. Consequently, the normalized probablity density function becomes:\n",
    "\n",
    "$\n",
    "\\quad \\tilde{f}_{S_N}(s; t)\n",
    "= \\dfrac{d \\tilde{F}_{S_N}(s; t)}{ds}\n",
    "= \\dfrac{1}{F_0} \\dfrac{d F_{S_N}(s; t)}{ds}\n",
    "$\n",
    "\n",
    "\n",
    "To find the expectation for $S_N$ when the $N^{th}$ event occurs, we multiply $s$ by the pdf and integrate over the domain of $s$:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\quad \\mathbb{E}_t(S_N)\n",
    "&= \\int_{-\\infty}^{\\infty} s \\tilde{f}_{S_N}(s; t) ds \\\\\n",
    "&= \\int_{0}^{\\infty} s \\bigg ( \\dfrac{1}{F_0} \\sum_{n=0}^{N-1} \\dfrac{1}{n!} \\Big [ e^{-\\Lambda(t,s)} \\lambda(t+s) \\Lambda(t,s)^{n-1}\n",
    "\\left ( \\Lambda(t,s) - n \\right ) \\Big ] \\bigg ) ds \\\\\n",
    "&= \\dfrac{1}{F_0} \\sum_{n=0}^{N-1} \\bigg ( \\dfrac{1}{n!} \\int_{0}^{\\infty} s \\Big [ e^{-\\Lambda(t,s)} \\lambda(t+s) \\Lambda(t,s)^{n-1}\n",
    "\\left ( \\Lambda(t,s) - n \\right ) \\Big ] ds \\bigg )\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected number of unobserved events\n",
    "We may encounter scenarios in which we wish to estimate the number of unobserved over some time frame, given that we have yet to observe a threshold event (i.e. the manifestation of $L$ hidden events). To do so, we incorporate the knowledge that the probability of observing $L$ or greater events in the time period is zero:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\quad P(N(s) = n \\mid N(s) < L; t)\n",
    "&= \\left \\{ \\begin{array}{cc}\n",
    "e^{-\\Lambda(t,s)} \\frac{\\Lambda(t,s)^n}{n!} & \\text{if}\\,\\, n < L \\\\\n",
    "0                                                         & \\text{if}\\,\\, n \\geq L\n",
    "\\end{array} \\right.\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "We incoporate the marginal probability $P_0$:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\quad P_0 \n",
    "&= \\sum_{n=0}^{\\infty} P(N(s) = n; t) \\\\\n",
    "&= \\sum_{n=0}^{L-1} P(N(s) = n; t) \\\\\n",
    "&= \\sum_{n=0}^{L-1} \\left ( e^{-\\Lambda(t,s)} \\dfrac{\\Lambda(t,s)^n}{n!} \\right )\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "in order to normalize the distribution:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\quad \\tilde{P}(N(s) = n \\mid N(s) < L; t)\n",
    "&= \\left \\{ \\begin{array}{cc}\n",
    "\\dfrac{e^{-\\Lambda(t,s)} \\frac{\\Lambda(t,s)^n}{n!}}{P_0}  & \\text{if}\\,\\, n < L \\\\\n",
    "0                                                         & \\text{if}\\,\\, n \\geq L\n",
    "\\end{array} \\right.\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Taken together, the expected value of unobserved events is simply the summation of the number of events multiplied by the probability of occurrence:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\quad E_t[N(s) \\mid N(s) < L]\n",
    "&= \\sum_{n=0}^{\\infty} n \\tilde{P}(N(s) = n; t) \\\\\n",
    "&= \\sum_{n=0}^{L-1} n \\tilde{P}(N(s) = n; t) \\\\\n",
    "&= \\dfrac{\\sum_{n=0}^{L-1} \\left ( n e^{-\\Lambda(t,s)} \\frac{\\Lambda(t,s)^n}{n!} \\right )}{\\sum_{n=0}^{L-1} \\left ( e^{-\\Lambda(t,s)} \\frac{\\Lambda(t,s)^n}{n!} \\right )}\n",
    "\\end{align}\n",
    "$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (behavior)",
   "language": "python",
   "name": "behavior"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
