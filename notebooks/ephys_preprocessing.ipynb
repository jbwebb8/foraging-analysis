{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Electrophysiology preprocessing\n",
    "This notebook will explore the basic pipeline for processing and analyzing neural data recorded using the Intan system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical tools\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# General purpose\n",
    "import os, re, glob, sys\n",
    "import struct\n",
    "import h5py\n",
    "import getpass\n",
    "from time import time\n",
    "import json\n",
    "\n",
    "# MountainSort, etc.\n",
    "from mountainlab_pytools import mdaio\n",
    "from mountainlab_pytools import mlproc as mlp\n",
    "from ml_ephys.preprocessing.p_bandpass_filter import bandpass_filter as ml_bandpass_filter\n",
    "from ml_ephys.preprocessing.p_whiten import whiten as ml_whiten\n",
    "#from ml_ms4alg.p_ms4alg import sort as ml_sort\n",
    "\n",
    "# Ephys code\n",
    "sys.path.insert(0, '../python/')\n",
    "import util\n",
    "import ephys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw ephys data\n",
    "raw_dir = '/media/james/mcginley_server_3/James/data/Ephys/03-08-19/intan/j7z5_d27_190308_152151/'\n",
    "\n",
    "# Preprocessed ephys data (Google Drive API)\n",
    "pre_dir = '/media/james/data/foraging/ephys/03-08-19/spike_sorting/mountainsort/shank_2/' # preprocessed data directory\n",
    "\n",
    "# Output\n",
    "results_dir = '/media/james/data/foraging/ephys/03-08-19/analysis/shank_1/' # directory to store downloads\n",
    "\n",
    "# Channels\n",
    "channel_ids = []\n",
    "channel_ids.append(np.arange(64, dtype=np.int16))\n",
    "channel_ids.append(np.arange(64, 128, dtype=np.int16))\n",
    "shank_id = 1 # shank to analyze (0-indexed!)\n",
    "\n",
    "# Spike sorting parameters\n",
    "rv_thresh = 1.0 # threshold for refractory violation period\n",
    "\n",
    "# Autocorrelogram\n",
    "dt_bin = 0.001\n",
    "dt_bin_max = 0.100\n",
    "\n",
    "# Waveform\n",
    "dt_wf = 0.005 # window size (s)\n",
    "fs = 30000 # sampling rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow read/write permissions from directory\n",
    "password = getpass.getpass()\n",
    "command = 'sudo -S chmod 777 %s && sudo -S chmod 777 %s' % (pre_dir, results_dir) # -S enables input from stdin\n",
    "os.system('echo %s | %s' % (password, command));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "raw_files = []\n",
    "ch_ids = []\n",
    "\n",
    "# Find raw files and associated channel IDs\n",
    "regexp = re.compile(r'amp-A-[0-9]+')\n",
    "for f in os.listdir(raw_dir):\n",
    "    if regexp.search(f):\n",
    "        raw_files.append(f)\n",
    "        ch_ids.append(int(f[6:9]))\n",
    "\n",
    "# Sort if necessary\n",
    "ch_ids = np.asarray(ch_ids)\n",
    "sort_idx = np.argsort(ch_ids)\n",
    "ch_ids = ch_ids[sort_idx]\n",
    "raw_files = [raw_files[i] for i in sort_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find all raw data files\n",
    "#raw_files = sorted(glob.glob(raw_dir + 'amp*.dat'))\n",
    "#ch_ids = np.array([int(f[6:9]) for f in raw_files])\n",
    "\n",
    "# Keep only channels to analyze\n",
    "channels = [i for i in ch_ids if i in channel_ids[shank_id]]\n",
    "raw_files = [raw_files[i] for i in channels]\n",
    "ch_ids = channels\n",
    "\n",
    "# Print filenames\n",
    "print('Files to load:')\n",
    "print('\\n'.join(raw_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data in numpy array\n",
    "X = []\n",
    "for i in range(len(raw_files)):\n",
    "    print('Processing file %02d of %02d...' % (i+1, len(raw_files)))\n",
    "    X.append(np.fromfile(raw_dir + raw_files[i], dtype=np.int16))\n",
    "    \n",
    "X = np.array(X)\n",
    "print(X.shape)\n",
    "print('total recording time: %.2f min' % (X.shape[1]/fs/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data in directory\n",
    "if not os.path.isdir(pre_dir + 'original/'):\n",
    "    os.mkdir(pre_dir + 'original/')\n",
    "mdaio.writemda16i(X, pre_dir + 'original/raw.mda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spike sorting\n",
    "### MountainSort\n",
    "Once the initial `.mda` file is converted from the raw `.dat` files, spikes can be extracted and sorted using `MountainSort`. The final product will be another `.mda` file that contains the firing data. Full documentation [here](https://github.com/flatironinstitute/mountainsort_examples/blob/master/README.md).\n",
    "\n",
    "The basic process requires four steps:\n",
    "1. **Bandpass filter**: typically between 600 Hz and 6000 Hz.\n",
    "2. **Whiten data spatially**: breaks noise correlations between channels.\n",
    "3. **Sort waveforms**.\n",
    "4. **Compute cluster metrics**: isolation, noise_overlap, firing_rate, snr_peak.\n",
    "5. **Semi-auto curation**: based on cluster metrics.\n",
    "\n",
    "#### Data preprocessing, spike detection, and sorting algorithm\n",
    "The associated steps in the command-line interface are (with recommended parameters):\n",
    "\n",
    "```\n",
    "ml-run-process ephys.bandpass_filter --inputs timeseries:<raw_filename>.mda  --outputs timeseries_out:<raw_filename>_bandpass.mda --parameters samplerate:30000 freq_min:<freq_min> freq_max:<freq_max>\n",
    "\n",
    "ml-run-process ephys.whiten --inputs timeseries:<raw_filename>_bandpass.mda  --outputs timeseries_out:<raw_filename>_whitened.mda\n",
    "\n",
    "ml-run-process ms4alg.sort --inputs timeseries:<raw_filename>_whitened.mda geom:<geom_filename>.csv --outputs firings_out:<firings_filename>.mda --parameters adjacency_radius:-1 detect_sign:-1 detect_threshold:4\n",
    "```\n",
    "\n",
    "Some notes about the above processes:\n",
    "- Docstrings for the functions called by the processor can be found in the [appendix](#Appendix).\n",
    "- To run sequentially, simply add `&&\\` after each line in terminal.\n",
    "- To use the geometry of the electrode configuration on the shank for spike sorting, we must first create a `.csv` file containing a list of coordinates for each electrode, such that each row is [channel_id, x, y, z].\n",
    "- Code for the processes can be found on github: [ephys](https://github.com/magland/ml_ephys), [ms4alg](https://github.com/magland/ml_ms4alg/tree/master/ml_ms4alg), [pipeline](https://github.com/magland/ml_ms4alg/blob/master/examples/ms4alg_pipelines.py)\n",
    "\n",
    "After running the above processes, the following files should have been created:\n",
    "- `<raw_filename>_bandpass.mda`: bandpass-filtered raw data [M channels x T samples]\n",
    "- `<raw_filename>_whitened.mda`: whitened, bandpass-filtered raw data [M channels x T samples]\n",
    "- `<firings_filename>.mda`: a [3 x N] array containing firing data with the following rows\n",
    "    - `channel_id`: the channel number associated with the spike\n",
    "    - `t_spike`: the sample number associated with the spike\n",
    "    - `cluster_id`: the unit number associated with the spike\n",
    "- `combined_metrics.json`: cluster metrics for every sorted unit\n",
    "\n",
    "#### Unit curation \n",
    "Although initially implied to be fully automated, the best practice is still to combine automated features with manual inspection. The workflow is as follows. First, compute the cluster metrics useful for curation (requires installing the `ml_ms3` module):\n",
    "\n",
    "```\n",
    "ml-run-process ms3.cluster_metrics --inputs firings:<firings_filename>.mda timeseries:<raw_filename>_whitened.mda --outputs cluster_metrics_out:cluster_metrics.json --parameters samplerate:30000\n",
    "\n",
    "ml-run-process ms3.isolation_metrics --inputs firings:<firings_filename>.mda timeseries:<raw_filename>_whitened.mda --outputs metrics_out:isolation_metrics.json pair_metrics_out:pair_metrics.json --parameters compute_bursting_parents:true\n",
    "\n",
    "ml-run-process ephys.compute_cluster_metrics --inputs firings:<firings_filename>.mda timeseries:<raw_filename>_whitened.mda --outputs metrics_out:cluster_metrics_ms4.json --parameters samplerate:30000 clip_size:100 refrac_msec:1.0\n",
    "\n",
    "ml-run-process ms3.combine_cluster_metrics --inputs metrics_list:cluster_metrics_ms3.json metrics_list:cluster_metrics_ms4.json metrics_list:isolation_metrics.json --outputs metrics_out:metrics.json\n",
    "```\n",
    "\n",
    "A quick note about `ephys.combine_cluster_metrics`. Unlike the other two metrics processes, this one does not filter out clusters that are no longer being analyzed (either through rejecting or merging). Instead, it assumes all clusters exist with labels $[1, ..., K]$, where $K$ is the largest label integer. Clusters that have disappeared with labels in that range will still be analyzed with sometimes uninterpretable metrics or metrics that are incompatible with JSON files (e.g. writing NaN). This will cause the last step (`ms3.combine_cluster_metrics`) to simply ignore that JSON file as input (but without raising any warning or error!). To correct for this issue, simply add the following line of code to the `p_compute_cluster_metrics.py` script in the `ml_ephys` module:\n",
    "\n",
    "```\n",
    "clusters = [cluster for cluster in clusters if cluster['metrics']['num_events'] > 0.0]\n",
    "```\n",
    "\n",
    "In the completely automated method, the next steps would be to run the `ms4alg.create_label_map` and `ms4alg.apply_label_map` processes from the command-line interface, which would ultimately create a `curated_firings.mda` file that removes and merges units from `firings.mda` based on cluster metrics. There are two issues with this. First, the cluster metrics, while useful, still do not entirely capture the complexity of clusters (e.g. drifting, etc.). Second, when cluster metrics are recomputed in the next pass of curation, they should still include the rejected units, which have been removed in the `curated_firings.mda` file (note, however, that the original `firings.mda` file won't work either, since it doesn't include merged units). To account for these issues, instead run the process `pyms.add_curation_tags` from the [Frank lab package](https://bitbucket.org/franklab/franklab_mstaggedcuration/src/master/). (For installation, see Appendix.) This will tag units that do not meet the cluster metric criteria with `mua`. Next, run `qt-mountainview` and apply own tags, looking for merges, multi-unit activity, drifts, etc. Once the initial pass is complete, run the process `pyms.merge_burst_parents` to merge units with bursting parents. In summary:\n",
    "\n",
    "```\n",
    "ml-run-process pyms.add_curation_tags --inputs metrics:metrics.json --outputs metrics_tagged:metrics.json  --parameters firing_rate_thresh:0.1 isolation_thresh:0.95 noise_overlap_thresh:0.03 peak_snr_thresh:1.5\n",
    "\n",
    "qt-mountainview\n",
    "\n",
    "ml-run-process pyms.merge_burst_parents\n",
    "```\n",
    "\n",
    "Now cluster metrics can once again be calculated, and the process is repeated until no additional changes are made. See below for more details about the steps surrounding MountainView.\n",
    "\n",
    "#### Automated sorting and curation\n",
    "Rather than having to run all steps in the command-line interface (and monitor for the progress of each process), we can use `ephys.sort_spikes(timeseries)` from my `ephys` module instead. This script automatically runs the above processes in order (filter, whiten, sort, calculate metrics, apply tags) with parameters specified in one of two ways: either in a JSON file (`config=filename`) or as individual keyword arguments (`**kwargs`). In either case, parameters not specified will be assigned the default values below:\n",
    "```\n",
    "'output_dir': './',\n",
    "'geom_filepath': '',\n",
    "'sample_rate': 30000,\n",
    "'freq_min': 300,\n",
    "'freq_max': 6000,\n",
    "'adjacency_radius': -1,\n",
    "'detect_sign': -1,\n",
    "'detect_threshold': 4,\n",
    "'bursting_parents': True,\n",
    "'clip_size': 100,\n",
    "'refrac_msec': 1.0,\n",
    "'firing_rate_thresh': 0.1,\n",
    "'isolation_thresh': 0.95,\n",
    "'noise_overlap_thresh': 0.03,\n",
    "'peak_snr_thresh': 1.5,\n",
    "'opts': {}\n",
    "```\n",
    "\n",
    "The names and values of these parameters are exactly the same as appears in the command-line interface. Note that the only exceptions are that initial inputs must be provided for a couple of processes (all other I/O is piped automatically):\n",
    "- `output_dir`: The output directory in which to store output files from all processes.\n",
    "- `geom_filepath`: The full filepath to the geometry `.csv` file needed for the sorting algorithm.\n",
    "\n",
    "Note that the location of the `ml-run-process` executable must be in the environment `$PATH` variable. If installed in a `conda` environment, for example, then jupyter must be started with that environment activated; otherwise, `$PATH` will not correspond that the `conda` environment. In other words, if using the environment `ms4js`, then simply run the following to start this notebook:\n",
    "\n",
    "```\n",
    "source activate ms4js\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "#### MountainView\n",
    "Start MountainView by running from the command-line with `qt-mountainview`. Unfortunately, this is about as far as the documentation goes (from what I can find), so let's write out the rest here. The command-line interface takes *named parameters* of the form `--<param_name>=<value>` and *unnamed parameters* as simply `<value>` (see [here](https://github.com/flatironinstitute/qt-mountainview/blob/master/cpp/mountainview/src/mountainviewmain.cpp#L237) and [here](https://github.com/flatironinstitute/qt-mountainview/blob/master/packages/mv/common/clparams.cpp) for code details). These are as follows:\n",
    "\n",
    "```\n",
    "unnamed parameters:\n",
    "  <filename>.mv\n",
    "  <filename>.mv2: cluster tags\n",
    "  <filename>.smv: MountainView configuration file\n",
    "  unit_test: possible values are remotereadmda, remotereadmda2, taskprogressview\n",
    "\n",
    "named parameters:\n",
    "--mode: possible values are overview2, spikespy\n",
    "--timeseries: (whitened) timeseries file (overview2, spikespy modes)\n",
    "--firings: firings mda file (overview2, spikespy modes)\n",
    "--samplerate: sample rate (Hz) (overview2, spikespy modes)\n",
    "--filt: filtered data (overview2 mode)\n",
    "--pre: preprocessed data (overview2 mode)\n",
    "--mlproxy_url (overview2 mode)\n",
    "--window_title (overview2 mode)\n",
    "--geom: csv file containing geometry of channels on probe (overview2 mode)\n",
    "--cluster_metrics: JSON file containing cluster metrics (overview2 mode)\n",
    "--curation: path to curation program (overview2 mode)\n",
    "--clusters: subset of clusters to view (overview2 mode); set as comma-separated list of ints (e.g. 1,5,8)\n",
    "```\n",
    "\n",
    "If following the above procedure, then a suggested command is:\n",
    "\n",
    "```\n",
    "qt-mountainview --timeseries=<raw_filename>_whitened.mda --firings=<firings_filename>.mda --samplerate=30000 --geom=<geometry_filename>.csv --cluster_metrics=combined_cluster_metrics.json\n",
    "```\n",
    "\n",
    "Once opened, begin to tag units appropriately, labeling as either `accepted` or otherwise (`noise`, `mua`, etc.). (Note that if the suggested workflow above has been done prior to using MountainView, then all units which do not meet acceptable criteria based on cluster metrics will already be labeled as `mua`.) There are many ways to approach manual unit curation. One might be:\n",
    "\n",
    "1. Look at the waveform templates initially. Be wary of atypical waveforms.\n",
    "2. For units whose largest amplitude lies on nearby channels, compute the cross-correlations and PCA features.\n",
    "3. For cross-correlation, units should have no overlap in the middle (refractory period) compared to themselves, and be relatively flat compared to nearby units. Two or more units that have sharp peaks near the middle may be related (e.g. bursting parents).\n",
    "4. Units should be well-isolated in PCA space with an ideally spherical distribution. When units overlap significantly, consider merging.\n",
    "5. Look at firing events to determine if units that display overlap in PCA space or interesting cross-correlation are related by drift over time.\n",
    "\n",
    "Once all units are tagged, export three files:\n",
    "\n",
    "1. `curation.mv2` (export .mv2): A MountainView file that will save all work done in the current session.\n",
    "2. `metrics_curated.json` (export cluster metrics): A JSON file containing cluster metrics will all new tags applied.\n",
    "3. `firings_merged.mda` (export curated firings): A file containing firings with new labels based on merges made during the current session.\n",
    "\n",
    "IMPORTANT: Do *not* label any units as `rejected`; instead, use other labels, such as `mua`, `noise`, etc. We need the exported firings to contain all units, including those that will be rejected, in case we need to do another round of curation, which requires re-computing cluster metrics again. Therefore, we export the firings from the MountainView session to include all units (but combine merged units), and then curate the firings file later (see below).\n",
    "\n",
    "Random notes:\n",
    "- How are cluster pairs selected for comparison? First, the waveform template for each cluster is compared to that of every other cluster by obtaining the squared distance (i.e. $||\\mathbf{X}-\\mathbf{Y}||^2$) and correlation between them. The three clusters with the lowest squared distances (i.e. most similar) and correlation scores >0.8 are added to the list of pair comparisons to perform. Then, for each pair in the list, the overlap in PC space is computed (which uses a k-nearest-neighbors search; see paper for details). The `overlap_cluster` metric computed in `ms3.isolation_metric` corresponds to the cluster with the highest overlap score.\n",
    "- When looking at waveforms, MountainView displays templates in *channel number* order (with channel 1 at the top), even if a `geom` file is provided! Thus channels close together on the physical probe may be displayed far apart, meaning that the largest waveforms for a particular cluster could be separated by many channels on the display. Also, when inspecting geometry, keep in mind 0- vs. 1-based indexing.\n",
    "    - 0-indexed: wiring diagram, my timeseries_reader code\n",
    "    - 1-indexed: sorted output (`firings.mda1`), MountainView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move above documentation to new repo in separate md file\n",
    "# Then move notebook and associated python scripts into same repo\n",
    "# Do spike sorting out of notebook; see how to run processes at https://github.com/magland/mountainlab_pytools/blob/master/test/test.ipynb\n",
    "# or better yet, create pipeline script like ms4_sort_dataset that pulls from mlproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "timeseries_filepath = pre_dir + 'original/raw.mda'\n",
    "config_filepath = '../config/sort.json'\n",
    "\n",
    "# Initialize pipeline\n",
    "Pipeline = mlp.initPipeline()\n",
    "\n",
    "# Run sorting script\n",
    "with Pipeline:\n",
    "    client = Pipeline._client\n",
    "    ephys.sort_spikes(timeseries=timeseries_filepath, config=config_filepath)\n",
    "    \n",
    "#ephys.track_pipeline_progress(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edit curation files\n",
    "After running the above script or using the command-line interface for MountainSort, and tagging clusters in MountainView, run the following to create `firings_curated.mda`, which contains spiking information of only clusters that have passed both automated (cluster metrics) and manual (MountainView) inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "firings_old_filepath = pre_dir + 'original/firings.mda' # firings before this curation iteration\n",
    "firings_merged_filepath = pre_dir + 'curation_1/firings_merged.mda' # merged firings from MountainView\n",
    "firings_curated_filepath = pre_dir + 'curation_1/firings_curated.mda' # location to store curated firings\n",
    "metrics_filepath = pre_dir + 'curation_1/metrics_curated.json' # metrics file with all tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create map of merges during curation\n",
    "ephys.create_merge_map(firings_old=firings_old_filepath, \n",
    "                       firings_new=firings_merged_filepath,\n",
    "                       merge_map_out=pre_dir + 'curation_1/merge_map.json');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Update new cluster metrics with tags added during curation\n",
    "#metrics_tagged_filepath = pre_dir + 'curation_1/metrics_curated_tagged.json'\n",
    "#ephys.update_cluster_tags(metrics_old=cluster_metrics_old_filepath,\n",
    "#                          metrics_new=cluster_metrics,\n",
    "#                          metrics_out=metrics_tagged_filepath);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create firings file of only accepted clusters for analysis\n",
    "ephys.curate_firings(firings=firings_merged_filepath,\n",
    "                     metrics=metrics_filepath,\n",
    "                     keep_tags='accepted',\n",
    "                     firings_out=firings_curated_filepath);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual inspection\n",
    "After automatically and manually curating the sorted clusters with MountainSort and MountainView, respectively, use this section to inspect the final results with tools such as autocorrelograms, waveform templates, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_filepath = pre_dir + 'original/raw_whitened.mda'\n",
    "firings_filepath = pre_dir + 'curation_1/firings_curated.mda'\n",
    "metrics_filepath = pre_dir + 'curation_1/metrics_curated.json'\n",
    "geom_filepath = pre_dir + '../ml_files/geom128-s1.csv'\n",
    "use_google_drive = False # if true, find files in Google Drive\n",
    "storage_mode = 1 # 0 = read from server, 1 = download to/read from local disk, 2 = load in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "if use_google_drive:\n",
    "    # Create Service object\n",
    "    drive_service = util.GoogleDriveService()\n",
    "    \n",
    "    # Load spike data\n",
    "    print('Loading spike data...', end=' ')\n",
    "    firings = util.bytes_to_object(drive_service.download(filename=firings_filepath), ob_type='numpy')\n",
    "    print('done.')\n",
    "    \n",
    "    # Load File object for reading timeseries MDA file\n",
    "    print('Creating File object for timeseries MDA file...', end=' ')\n",
    "    if storage_mode == 0:\n",
    "        # To save storage, can continually read from server, but this can be prohibitively slow\n",
    "        # if many reads are required\n",
    "        drive_file = util.GoogleDriveFile(drive_service, timeseries_filepath)\n",
    "        timeseries_reader = util.MDAReader(drive_file)\n",
    "    elif storage_mode == 1:\n",
    "        # Faster to download entire file and store on hard disk, but files can be large (~50+ GB)\n",
    "        if not os.path.exists(media_dir + timeseries_filepath):\n",
    "            f = open(media_dir + timeseries_filepath, 'w+b')\n",
    "            drive_service.download(filename=timeseries_filepath, file_object=f)\n",
    "            f.seek(0, 0) # reset pointer to beginning of file\n",
    "        else:\n",
    "            f = open(media_dir + timeseries_filepath, 'rb')\n",
    "        timeseries_reader = util.MDAReader(f)\n",
    "    elif storage_mode == 2:\n",
    "        # Fastest to load entirely in memory\n",
    "        f = io.BytesIO(drive_service.download(filename=timeseries_filepath))\n",
    "        timeseries_reader = util.MDAReader(f)\n",
    "    else:\n",
    "        raise SyntaxError('Unknown storage mode %d.' % storage_mode)\n",
    "    print('done.')\n",
    "        \n",
    "    # Load cluster metrics\n",
    "    cluster_metrics = util.bytes_to_object(drive_service.download(filename=metrics_filepath), ob_type='json')\n",
    "    \n",
    "else:\n",
    "    # Load spike data\n",
    "    with open(firings_filepath, 'rb') as f:\n",
    "        firings = util.MDAReader(f).read()\n",
    "    \n",
    "    # Load File object for reading timeseries MDA file\n",
    "    if storage_mode in [0, 1]:\n",
    "        timeseries_reader = util.MDAReader(open(timeseries_filepath, 'rb'))\n",
    "    elif storage_mode == 2:\n",
    "        timeseries_reader = io.BytesIO(open(timeseries_filepath, 'rb'))\n",
    "        \n",
    "    # Load cluster metrics\n",
    "    with open(metrics_filepath, 'r') as f:\n",
    "        cluster_metrics = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autocorrelogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute autocorrelogram\n",
    "isi = [] # inter-spike intervals\n",
    "for i, unit in enumerate(cluster_ids):\n",
    "    print('Processing unit %d (%d of %d)...' % (unit, i+1, len(cluster_ids)), end=' ')\n",
    "    \n",
    "    # Get spike times during behavior\n",
    "    t_spikes_ = t_spikes[spikes[2, :] == unit]\n",
    "    t_spikes_ = t_spikes_[t_spikes_ > 0]\n",
    "    num_spikes = len(t_spikes_)\n",
    "\n",
    "    # Placeholders\n",
    "    idx = np.arange(num_spikes) # indices to compute ISI\n",
    "    j = 1 # number of spikes ahead to compute ISI\n",
    "    isi_ = [] # inter-spike intervals for unit\n",
    "    while True:\n",
    "        # Filter indices by max ISI for given step size\n",
    "        idx = idx[idx + j < num_spikes] # handle end case\n",
    "        idx = idx[t_spikes_[idx + j] - t_spikes_[idx] <= dt_bin_max] # limit max ISI\n",
    "\n",
    "        # Compute ISI for given step size\n",
    "        if idx.size > 0:\n",
    "            isi_j = t_spikes_[idx + j] - t_spikes_[idx]\n",
    "            isi_.append(isi_j)\n",
    "            isi_.append(-isi_j)\n",
    "            j += 1\n",
    "            # No need to reset idx, since j has at least as many as j+1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Save flattened list of ISIs\n",
    "    isi.append(flatten_list(isi_))\n",
    "    \n",
    "    print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "cols = 4\n",
    "rows = (num_units // cols) + (num_units % cols)\n",
    "dt_refrac = 0.001 # refractory period (s)\n",
    "num_bins = dt_bin_max // dt_bin\n",
    "bin_edges = np.arange(-num_bins, num_bins+1) * dt_bin\n",
    "\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(15, 3*rows))\n",
    "\n",
    "for i, unit in enumerate(cluster_ids):\n",
    "    print('Processing unit %d (%d of %d)...' % (unit, i+1, len(cluster_ids)), end=' ')\n",
    "    \n",
    "    # Set axis indices\n",
    "    j = i // cols\n",
    "    k = i % cols\n",
    "    \n",
    "    # Plot histogram and outline refractory period\n",
    "    hist, _, _ = ax[j, k].hist(isi[i], bins=bin_edges)\n",
    "    _ = ax[j, k].fill_between(np.array([-dt_refrac, dt_refrac]), y1=0.0, y2=np.max(hist),\n",
    "                              alpha=0.2, color='black')\n",
    "    \n",
    "    # Axis settings\n",
    "    ax[j, k].set_title('unit %d' % unit)\n",
    "    ax[j, k].set_xlabel('ISI (s)')\n",
    "    ax[j, k].set_ylabel('count')\n",
    "    \n",
    "    print('done.')\n",
    "\n",
    "# Plot settings\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = np.loadtxt(geom_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_labels, wfs = ephys.get_templates(timeseries=timeseries_reader,\n",
    "                                          firings=firings,\n",
    "                                          templates_out=pre_dir + 'curation_1/templates_curated.npy',\n",
    "                                          window=[-2.5, 2.5],\n",
    "                                          f_s=30000,\n",
    "                                          scale=0.195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ephys.plot_templates(labels=cluster_labels, \n",
    "                     templates=wfs,\n",
    "                     fig_out=pre_dir+'curation_1/templates_curated.pdf',\n",
    "                     geom=geom, \n",
    "                     plot_style='geometric', \n",
    "                     metrics=cluster_metrics);\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = [31, 32, 33]\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    print('Processing unit %d (%d of %d)...' % (label, i+1, len(labels)), end=' ')\n",
    "    \n",
    "    idx = np.where(cluster_labels == label)[0][0]\n",
    "    fig, ax = ephys.plot_templates(labels=cluster_labels[idx:idx+1], \n",
    "                                   templates=wfs[idx:idx+1],\n",
    "                                   geom=geom, \n",
    "                                   plot_style='vertical', \n",
    "                                   metrics=cluster_metrics,\n",
    "                                   fig_size=(5, 15));\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    plt.savefig(pre_dir + 'curation_1/templates_curated_unit_%d.pdf' % label)\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine data\n",
    "If preprocessing multiple shanks, combine their firing and metrics data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "base_dir = '/media/james/data/foraging/ephys/03-08-19/spike_sorting/mountainsort/'\n",
    "out_dir = '/media/james/data/foraging/ephys/03-08-19/spike_sorting/mountainsort/all/'\n",
    "firings_filelist = [base_dir + 'shank_1/curation_1/firings_curated.mda',\n",
    "                    base_dir + 'shank_2/curation_1/firings_curated.mda']\n",
    "metrics_filelist = [base_dir + 'shank_1/curation_1/metrics_curated.json',\n",
    "                    base_dir + 'shank_2/curation_1/metrics_curated.json']\n",
    "shank_ids =[0, 1]\n",
    "ch_offset = [0, 64]\n",
    "\n",
    "# Check lists\n",
    "if not (len(firings_filelist) == len(metrics_filelist) == len(shank_ids) == len(ch_offset)):\n",
    "    raise SyntaxError('List lengths are not equal.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firings_all = []\n",
    "n_firings = 0\n",
    "metrics_all = []\n",
    "for i in range(len(firings_filelist)):\n",
    "    # Load spike data\n",
    "    with open(firings_filelist[i], 'rb') as f:\n",
    "        firings = util.MDAReader(f).read()\n",
    "    \n",
    "    # Update cluster channels and labels\n",
    "    firings[0, :] += ch_offset[i]\n",
    "    firings[2, :] += shank_ids[i]*1000\n",
    "    n_firings += firings.shape[1]\n",
    "    \n",
    "    # Load cluster metrics\n",
    "    with open(metrics_filelist[i], 'r') as f:\n",
    "        metrics = json.loads(f.read())\n",
    "        \n",
    "    # Update cluster labels\n",
    "    for j in range(len(metrics['clusters'])):\n",
    "        metrics['clusters'][j]['label'] += shank_ids[i]*1000\n",
    "    \n",
    "    # Stash updated data\n",
    "    firings_all.append(firings)\n",
    "    metrics_all.append(metrics)\n",
    "\n",
    "# Combine firings data\n",
    "firings_all_ = np.zeros([firings.shape[0], n_firings])\n",
    "j = 0\n",
    "for i, firings in enumerate(firings_all):\n",
    "    firings_all_[:, j:j+firings.shape[1]] = firings\n",
    "    j += firings.shape[1]\n",
    "firings_all = firings_all_\n",
    "sort_idx = np.argsort(firings_all[1, :])\n",
    "firings_all = firings_all[:, sort_idx]\n",
    "\n",
    "# Combine metrics data\n",
    "metrics_all_ = {}\n",
    "for metrics in metrics_all:\n",
    "    for k, v in metrics.items():\n",
    "        if k in metrics_all_.keys():\n",
    "            metrics_all_[k] += v\n",
    "        else:\n",
    "            metrics_all_[k] = util._check_list(v)\n",
    "metrics_all = metrics_all_\n",
    "\n",
    "# Save updated firings and metrics data\n",
    "mdaio.writemda64(firings_all, out_dir + 'firings.mda')\n",
    "with open(out_dir + 'metrics.json', 'w') as f:\n",
    "    json.dump(metrics_all, f, indent=4, sort_keys=True)\n",
    "    \n",
    "# Save location of datasets\n",
    "config = {'firings': firings_filelist,\n",
    "          'metrics': metrics_filelist,\n",
    "          'shank_ids': shank_ids,\n",
    "          'ch_offset': ch_offset}\n",
    "with open(out_dir + 'config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFP preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syncing Intan and Labview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Behavior data\n",
    "labview_filepath = '/media/james/mcginley_server_3/James/data/Ephys/03-08-19/labview/j7z5_d27_2019_03_08_15_22_24.mat'\n",
    "sess_filepath = '/media/james/data/foraging/head_fixed/j7z5/sess_27.p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sync data\n",
    "More specifics about the data types in the Intan files (header file, data files, etc.) can be found [here](http://intantech.com/files/Intan_RHD2000_data_file_formats.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sync pulse from labview DAQ\n",
    "with h5py.File(labview_filepath, 'r') as f:\n",
    "    labview_sync = np.squeeze(f['UntitledIntanSync']['Data'])\n",
    "    print('Labview sync size: %d (range %.2f to %.2f)' \n",
    "          % (labview_sync.shape[0], np.min(labview_sync), np.max(labview_sync)))\n",
    "    \n",
    "# Get sync pulse from Intan board\n",
    "intan_sync = np.fromfile(raw_dir + 'board-ADC-07.dat', dtype=np.uint16)\n",
    "intan_sync = intan_sync.astype(np.float64) * 0.000050354 # convert to volts\n",
    "print('Intan sync size:   %d (range %.2f to %.2f)' \n",
    "      % (intan_sync.shape[0], np.min(intan_sync), np.max(intan_sync)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check LabVIEW sample rate\n",
    "with h5py.File(labview_filepath, 'r') as f:\n",
    "    f_s_labview = int(1.0 / f['UntitledIntanSync']['Property']['wf_increment'][0])\n",
    "\n",
    "# Check Intan sample rate\n",
    "with open(raw_dir + 'info.rhd', 'rb') as f:\n",
    "    # Intan header\n",
    "    header, = struct.unpack('I', f.read(4))\n",
    "    \n",
    "    # Version number\n",
    "    version_major, = struct.unpack('h', f.read(2))\n",
    "    version_minor, = struct.unpack('h', f.read(2))\n",
    "    \n",
    "    # Sample rate\n",
    "    f_s_intan = int(struct.unpack('f', f.read(4))[0])\n",
    "\n",
    "print('Labview sample rate: {:.2f}'.format(f_s_labview))\n",
    "print('Intan sample rate:   {:.2f}'.format(f_s_intan))\n",
    "\n",
    "# If different, up/downsample intan pulse accordingly\n",
    "#if f_s_labview != f_s_intan:\n",
    "#    x = np.arange(labview_sync.shape[0])\n",
    "#    intan_sync = np.interp(x, x * f_s_intan / f_s_labview, intan_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate timestamps from sync pulses\n",
    "The labview clock will always be the \"ground truth\" for our purposes; any other time-series data (e.g. neural, pupil) will be aligned to it. Thus \"ground truth\" time is simply `idx * sample_rate` for the labview DAQ data. Timestamps for the sync pulses are aligned, and the timestamp associated with a given intan sample is set to the \"ground truth\" timestamp of the corresponding labview sample, with interpolation between sync pulses. Timestamps associated with intan samples occurring either before labview started or after it ended will be set to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine threshold crossings in each file to get timestamps\n",
    "thresh = 2.5\n",
    "\n",
    "is_pulse = (labview_sync > thresh).astype(np.int32)\n",
    "idx_labview = (np.argwhere((is_pulse - np.roll(is_pulse, -1)) == -1) + 1).flatten()\n",
    "\n",
    "is_pulse = (intan_sync > thresh).astype(np.int32)\n",
    "idx_intan = (np.argwhere((is_pulse - np.roll(is_pulse, -1)) == -1) + 1).flatten()\n",
    "\n",
    "# Check if number of pulses is the same\n",
    "is_equal = (idx_labview.shape[0] == idx_intan.shape[0])\n",
    "print('Number of pulses is %sequal.' % ('' if is_equal else 'not '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove intan data before and after labview\n",
    "idx_start = int(idx_intan[0] - (f_s_intan / f_s_labview) * idx_labview[0])\n",
    "idx_stop = int(idx_intan[-1] + (f_s_intan / f_s_labview) * (labview_sync.shape[0] - idx_labview[-1]))\n",
    "\n",
    "# Generate timestamps for each Intan sample by interpolating\n",
    "# between sync pulses. Timestamps for intan samples occurring\n",
    "# when labview was not running are set to -1.\n",
    "t_labview = np.arange(labview_sync.shape[0]) / f_s_labview\n",
    "t_intan = -np.ones(intan_sync.shape[0])\n",
    "x = np.append(np.insert(idx_intan, 0, idx_start), idx_stop)\n",
    "y = np.append(np.insert(idx_labview, 0, 0), labview_sync.shape[0]) / f_s_labview\n",
    "t_intan[idx_start:idx_stop] = np.interp(np.arange(idx_start, idx_stop), x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save timestamps\n",
    "data_dir = '/media/james/data/foraging/ephys/03-08-19/general/'\n",
    "np.save(data_dir + 'timestamps_daq_clock.npy', t_intan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MountainSort functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions in the `ephys` and `ms4alg` modules are written in Python with detailed docstrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ephys.bandpass_filter:')\n",
    "print(ml_bandpass_filter.__doc__, end='\\n\\n')\n",
    "\n",
    "print('ephys.whiten:')\n",
    "print(ml_whiten.__doc__, end='\\n\\n')\n",
    "\n",
    "print('ms4alg.sort:')\n",
    "print(ml_sort.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ms3` module is written in C++; the original code is in a separate [repo](https://github.com/flatironinstitute/mountainsort/tree/master/packages/ms3), containing [ms3.cluster_metrics](https://github.com/flatironinstitute/mountainsort/blob/master/packages/ms3/p_cluster_metrics.cpp), [ms3.isolation_metrics](https://github.com/flatironinstitute/mountainsort/blob/master/packages/ms3/p_isolation_metrics.cpp), and [ms3.combine_cluster_metrics](https://github.com/flatironinstitute/mountainsort/blob/master/packages/ms3/p_combine_firing_segments.cpp).\n",
    "\n",
    "When using the MountainLab processor (running `ml-run-process` on the command-line), the flags to pass for these functions are:\n",
    "\n",
    "```\n",
    "ms3.cluster_metrics\n",
    "--inputs\n",
    "    timeseries: <raw_filename>_whitened.mda\n",
    "    firings: <firings_filename>.mda\n",
    "--outputs\n",
    "    cluster_metrics_out: cluster_metrics.json\n",
    "--parameters\n",
    "    samplerate: <sample_rate>\n",
    "\n",
    "ms3.isolation_metrics\n",
    "--inputs\n",
    "    timeseries: <raw_filename>_whitened.mda\n",
    "    firings: <firings_filename>.mda \n",
    "--outputs\n",
    "    cluster_metrics_out: isolation_metrics.json\n",
    "--parameters\n",
    "    samplerate: <sample_rate>\n",
    "\n",
    "ms3.combine_cluster_metrics\n",
    "--inputs\n",
    "    metrics_list: cluster_metrics.json isolation_metrics.json\n",
    "--outputs\n",
    "    metrics_out: combined_metrics.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, although the `pyms` package contains functions in the `curation` directory that automatically apply curation based on cluster metrics (`create_label_map` and `apply_label_map`), I had difficulty installing them, so I created one myself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the additional processes from the Frank lab, follow these steps:\n",
    "\n",
    "```shell\n",
    "source activate <conda_env>\n",
    "git clone https://bitbucket.org/franklab/franklab_mstaggedcuration/src/master/\n",
    "```\n",
    "\n",
    "Before we install the code, we first need to make a couple of modifications. First, in order to know what datatype to expect for the process, the `mountainlab_pytools` module reads from the [function docstring](https://github.com/magland/mountainlab_pytools/blob/f2fe2e4f62fefa0969788e437628d293b7c30bcf/mountainlab_pytools/processormanager/processormanager_impl.py#L78-L113). Importantly, the format must be `<arg : datatype>`, where `arg` is the argument name, and `datatype` is `INPUT`, `OUTPUT`, or a parameter datatype (`int`, `float`, `float64`, or `string`). Change the docstring for `add_curation_tags` in `p_add_curation_tags.py` to the following:\n",
    "\n",
    "```\n",
    "    Add tags to the metrics file to reflect which clusters should be\n",
    "    rejected based on curation criteria.\n",
    "\n",
    "    Based on create/apply label map by J Chung and J Magland.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metrics : INPUT\n",
    "        Input file path of metrics json file to add tags\n",
    "\n",
    "    metrics_tagged : OUTPUT\n",
    "        Output file path of metrics json to be updated with cluster tags.\n",
    "\n",
    "    firing_rate_thresh : float64\n",
    "        Firing rate must be above this\n",
    "    isolation_thresh : float64\n",
    "        Isolation must be above this\n",
    "    noise_overlap_thresh : float64\n",
    "        `noise_overlap_thresh` must be below this\n",
    "    peak_snr_thresh : float64\n",
    "        peak snr must be above this\n",
    "    mv2file : string\n",
    "        If tags have already been added, update new metrics file with them\n",
    "```\n",
    "\n",
    "Secondly, the function must return `True` upon completion, so simply add `return True` at the end of the function (it currently does not return anything, or `None`).\n",
    "\n",
    "Now we can install the module:\n",
    "\n",
    "```shell\n",
    "cd franklab_mstaggedcuration\n",
    "pip install .\n",
    "```\n",
    "\n",
    "Next, make the module installed in your python environment visible in the MountainLab package directory. You can do this by either copying the repo:\n",
    "\n",
    "```shell\n",
    "cp -r franklab_mstaggedcuration/ $(ml-config package_directory)\n",
    "```\n",
    "\n",
    "or by creating a symbolic link to your installed package:\n",
    "\n",
    "```shell\n",
    "ln -s <path_to_package> $(ml-config package_directory)/franklab_mstaggedcuration\n",
    "```\n",
    "\n",
    "Now you should see the `pyms.add_curation_tags` and `pyms.merge_burst_parents` when you run `ml-list-processors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ms4js)",
   "language": "python",
   "name": "ms4js"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
